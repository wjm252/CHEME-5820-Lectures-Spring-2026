{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee27808e-d412-4e47-ba3e-3783e9c4f296",
   "metadata": {},
   "source": [
    "# Activity: Understanding Boltzmann Machines\n",
    "\n",
    "In this activity, we will explore some questions surrounding the training of a _small_ Boltzmann machine. In particular, we'll look at one of the key limitations of the training approach, namely requiring convergence to a stationary distribution for each training iteration.\n",
    "\n",
    "__Why are we looking at this?__ The [Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine) popularized by [Prof. G. Hinton in the mid-1980s](https://en.wikipedia.org/wiki/Geoffrey_Hinton) was (jointly) awarded the [2024 Nobel Prize in Physics](https://www.nobelprize.org/prizes/physics/2024/summary/) along with [Prof. J. Hopfield](https://en.wikipedia.org/wiki/John_Hopfield). While of little practical use (because of an issue that we will discuss today), these ideas led to the development of [the restricted Boltzmann machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine), which has many practical applications.\n",
    "\n",
    "> __Learning Objectives__\n",
    ">\n",
    "> By the end of this lab, you should be able to:\n",
    "> * __Sample a Boltzmann machine:__ Implement Gibbs sampling to generate state configurations from a Boltzmann machine and understand the role of the inverse temperature parameter $\\beta$.\n",
    "> * __Compute the stationary distribution:__ Calculate the exact Boltzmann distribution for a small network by enumerating all configurations and computing the partition function.\n",
    "> * __Compare empirical and theoretical distributions:__ Estimate the empirical distribution from samples and compare it to the theoretical stationary distribution to verify convergence.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c091b",
   "metadata": {},
   "source": [
    "## Background: What is a Boltzmann Machine?\n",
    "A [Boltzmann Machine](https://en.wikipedia.org/wiki/Boltzmann_machine) consists of a set of binary units (neurons, nodes, vertices, etc.) that are fully connected, with no self-connections. Formally, [a Boltzmann Machine](https://en.wikipedia.org/wiki/Boltzmann_machine) $\\mathcal{B}$ is a fully connected _undirected weighted graph_ defined by the tuple $\\mathcal{B} = \\left(\\mathcal{V},\\mathcal{E}, \\mathbf{W},\\mathbf{b}, \\mathbf{s}\\right)$.\n",
    "* __Units__: Each unit (vertex, node, neuron) $v_{i}\\in\\mathcal{V}$ has a binary state (`on` or `off`) and a bias value \n",
    "$b_{i}\\in\\mathbb{R}$, where $b_{i}$ is the bias of the node $v_{i}$. The bias vector $\\mathbf{b}\\in\\mathbb{R}^{|\\mathcal{V}|}$ is the vector of bias values for all nodes in the network. \n",
    "* __Edges__: Each edge $e\\in\\mathcal{E}$ has a weight. The weight of the edge connecting $v_{i}\\in\\mathcal{V}$ and $v_{j}\\in\\mathcal{V}$, is denoted by $w_{ij}\\in\\mathbf{W}$, where the weight matrix $\\mathbf{W}\\in\\mathbb{R}^{|\\mathcal{V}|\\times|\\mathcal{V}|}$ is symmetric, i.e. $w_{ij} = w_{ji}$ and $w_{ii} = 0$ (no self loops). The weights $w_{ij}\\in\\mathbb{R}$ determine the strength of the connection between the two nodes. \n",
    "* __States__: The state of each node is represented by a binary vector $\\mathbf{s}\\in\\mathbb{R}^{|\\mathcal{V}|}$, where $s_{i}\\in\\{-1,1\\}$ is the state of node $v_{i}$. When $s_{i} = 1$, the node is `on`, and when $s_{i} = -1$, the node is `off`. The set of all possible state _configurations_ is denoted by $\\mathcal{S} \\equiv \\left\\{\\mathbf{s}^{(1)},\\mathbf{s}^{(2)},\\ldots,\\mathbf{s}^{(N)}\\right\\}$, where $N$ is the number of possible state configurations, or $N = 2^{|\\mathcal{V}|}$ for binary units.\n",
    "\n",
    "Suppose we have values for the weights $\\mathbf{W}$ and biases $\\mathbf{b}$ of the Boltzmann machine. One of the key questions we can ask is: how can we generate samples from this Boltzmann machine?\n",
    "\n",
    "### Sampling algorithm\n",
    "One of the key theoretical ideas of [the Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine) is that the samples generated from it are distributed according to [the Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution). Let's test this idea. \n",
    "\n",
    "To generate samples from a Boltzmann Machine, let us consider the following algorithm (Gibbs sampling): \n",
    "\n",
    "__Initialize__ the weights $\\mathbf{W}$ and biases $\\mathbf{b}$ of the Boltzmann Machine. Provide an initial state $\\mathbf{s}^{(0)}$ of the network, and a system temperature $\\beta$.\n",
    "\n",
    "For each turn $t=1,2,\\dots,T$:\n",
    "1. For each node $v_{i}\\in\\mathcal{V}$:\n",
    "    1. Compute the total input $h_{i}^{(t)}$ to node $v_{i}$ using $h_{i}^{(t)} = \\sum_{j\\in\\mathcal{V}} w_{ij}s_{j}^{(t-1)} + b_{i}$.\n",
    "    2. Compute the probability of the _next_ state $s_{i}^{(t)} = 1$ using the logistic function $P(s_{i}^{(t)} = 1|h_{i}^{(t)}) = \\left(1+\\exp(-\\beta\\cdot{h}_{i}^{(t)})\\right)^{-1}$ for node $v_{i}$. The probability of $s_{i}^{(t)} = -1$ is given by $P(s_{i}^{(t)} = -1|h_{i}^{(t)}) = 1 - P(s_{i}^{(t)} = 1|h_{i}^{(t)})$.\n",
    "    3. Sample the _next_ state of node $v_{i}$ from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with parameter $p = P(s_{i}^{(t)} = 1|h_{i}^{(t)})$.\n",
    "2. Store the state vector $\\mathbf{s}^{(t)}$ of the network at turn $t$, and proceed to the next turn.\n",
    "\n",
    "\n",
    "Let's implement this algorithm, and see what happens!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7563b",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants.\n",
    "\n",
    "> The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902a4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\"); # load a bunch of libs, including the ones we need to work with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4dbf2",
   "metadata": {},
   "source": [
    "__Constants__: Set some constants that we will use later. Please look at the comments in the code for more details on each constant's permissible values, units, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = 3; # number of nodes in the system\n",
    "β = 0.1; # inverse temperature parameter for the system (big: cold, small: hot)\n",
    "number_of_turns = 10000; # number of turns that we take in the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88fbc19",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We need a helper function to compute the energy of a given state configuration. We've implemented this in the `energy(...)` method below.\n",
    "\n",
    "> The `energy(...)` method computes the energy of a state configuration $\\mathbf{s}$ using the formula $E(\\mathbf{s}) = -\\frac{1}{2}\\mathbf{s}^{\\top}\\mathbf{W}\\mathbf{s} - \\mathbf{b}^{\\top}\\mathbf{s}$. The first term captures pairwise interactions between nodes (weighted by $\\mathbf{W}$), and the second term captures the contribution of each node's bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73405daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "energy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function energy(model::MySimpleBoltzmannMachineModel, s::Vector{Int})::Float64\n",
    "\n",
    "    # initialize -\n",
    "    W = model.W; # weight matrix\n",
    "    b = model.b; # bias vector\n",
    "    energy = -(1/2)*dot(s, W*s) - dot(b, s); # compute the energy of the state\n",
    "\n",
    "    # return -\n",
    "    return energy;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f542b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0657cb9",
   "metadata": {},
   "source": [
    "## Task 1: Estimate the stationary distribution for a small system\n",
    "In this task, we sample the dynamics of a three-state [Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine) and explore a key question: can we estimate the stationary distribution of the Boltzmann machine from sample? \n",
    "\n",
    "We'll do this with a straightforward three-node Boltzmann machine (small enough to compute all the configurations required by the partition function) using the Gibbs sampling algorithm described above.\n",
    "\n",
    "First, let's set up our model of the Boltzmann machine with some random parameters that we hypothetically learned from data. For now, let's save the random weights in the `W::Array{Float64,2}` matrix and the random biases in the `b::Array{Float64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb2828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = let \n",
    "    \n",
    "    # initialize some random weights and biases\n",
    "    W = 2*randn(number_of_nodes, number_of_nodes);\n",
    "    b = randn(number_of_nodes);\n",
    "\n",
    "    # subract the mean from the weights (no self connections)\n",
    "    W = W - diagm(diag(W));\n",
    "\n",
    "    # return -\n",
    "    W, b\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b1fd5",
   "metadata": {},
   "source": [
    "__Model__: Next, let's build a model of the test Boltzmann machine. We'll use [the `MySimpleBoltzmannMachineModel` struct](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/boltzmann/#VLDataScienceMachineLearningPackage.MySimpleBoltzmannMachineModel) to represent the machine; we build an instance of this type [using a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/). The struct will have `W::Array{Float64,2}` and `b::Array{Float64,1}` fields that we set when we build an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd1a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build(MySimpleBoltzmannMachineModel, (\n",
    "    W = W,\n",
    "    b = b,\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0ac5a",
   "metadata": {},
   "source": [
    "Next, let's sample the dynamics of the Boltzmann machine using the Gibbs sampling algorithm described above. We've implemented the Gibbs sampling approach in [the `sample(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/boltzmann/#VLDataScienceMachineLearningPackage.sample-Tuple{MySimpleBoltzmannMachineModel,%20Vector{Int64}}) which implements the simple Gibbs sampling procedure described above.\n",
    "> The [`sample(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/boltzmann/#VLDataScienceMachineLearningPackage.sample-Tuple{MySimpleBoltzmannMachineModel,%20Vector{Int64}}) takes a [`MySimpleBoltzmannMachineModel` instance](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/boltzmann/#VLDataScienceMachineLearningPackage.MySimpleBoltzmannMachineModel), an initial state vector `sₒ::Array{Int,1}`,the number of turns `T::Int`, and a system (inverse) temperature `β::Float64`. The method returns an array of samples `S::Array{Int,2}` of size `N` $\\times$ `T,` where `N` is the number of nodes in the Boltzmann machine and `T` is the number of turns. \n",
    "\n",
    "What's in the `S::Array{Int,2}` array? Each column `S[:,t]` is the state vector of the network at turn `t`, i.e., `S[:,t] == s^(t)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdf255",
   "metadata": {},
   "outputs": [],
   "source": [
    "S, energy_state_array = let\n",
    "\n",
    "    # initialize the system\n",
    "    N = 2^number_of_nodes; # how many configurations do we have\n",
    "    energy_state = zeros(N); # energy of each state\n",
    "    \n",
    "    for i ∈ 0:(N - 1)\n",
    "        sᵢ = digits(i, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse # convert integer to binary state vector\n",
    "        energy_state[i + 1] = energy(model, sᵢ); # calculate the energy of each state\n",
    "    end\n",
    "    \n",
    "    start_energy_state = rand(1:N); # Heuristic: find the state with the minimum (maximum) or random energy\n",
    "    sₒ = digits(start_energy_state - 1, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse # convert to -1,1\n",
    "    S = VLDataScienceMachineLearningPackage.sample(model, sₒ, T = number_of_turns, β = β); # simulate the model \n",
    "\n",
    "    # return the data (we don't need the turn vector)``\n",
    "    S,  energy_state;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19e521",
   "metadata": {},
   "source": [
    "What is the lowest energy state of the Boltzmann machine defined by our (random) weights `W` and biases `b`? Let's look at the entries in the `energy_state_array` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9813a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Float64}:\n",
       " -0.3737561122973876\n",
       " -3.5158941104528116\n",
       " -0.5808517382568235\n",
       "  0.8610056589990673\n",
       "  1.7961132612247024\n",
       " -3.2117093805781067\n",
       "  2.724459833876427\n",
       "  2.300632587484933"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "energy_state_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33713892",
   "metadata": {},
   "source": [
    "__Check__: Let's verify how [the `digits(...)` method](https://docs.julialang.org/en/v1/base/numbers/#Base.digits) converts an integer to a binary state vector. The lowest energy state corresponds to the minimum entry in the `energy_state_array` array. Suppose this occurs at index `i_min`. We can convert this index to a binary state vector using the `digits(...)` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6796c969-1052-464d-a9de-97d57b41d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum energy state: [-1, -1, 1] with energy -3.5158941104528116 at index 1\n",
      "[-1, -1, 1] with energy -3.5158941104528116 at index 1\n"
     ]
    }
   ],
   "source": [
    "i_min = argmin(energy_state_array) - 1; # index of the minimum energy state\n",
    "sᵢ = digits(i_min, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse\n",
    "println(\"Minimum energy state: \", sᵢ, \" with energy \", energy_state_array[i_min + 1], \" at index \", i_min);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607d5b5",
   "metadata": {},
   "source": [
    "The low energy state is some distribution of the form `[s₁, s₂, s₃]`, where each `sᵢ` is either `-1` or `1`. This state corresponds to the configuration of the Boltzmann machine that has the lowest energy according to the weights and biases we defined earlier. However, should we expect to see this state often when we sample from the Boltzmann machine?\n",
    "\n",
    "> __Not necessarily!__ While low energy states are more probable according to the Boltzmann distribution, the actual frequency of observing a particular state during sampling depends on several factors, including the temperature parameter $\\beta$ and the structure of the energy landscape defined by the weights and biases. \n",
    "> * __High temperature__: At higher temperatures (lower $\\beta$), the system is more likely to explore higher energy states, leading to a more uniform distribution of observed states. \n",
    "> * __Low temperature__: Conversely, at lower temperatures (higher $\\beta$), the system tends to favor lower energy states, increasing the likelihood of observing the minimum energy state more frequently.\n",
    "> \n",
    "> What do we see when we sample from our Boltzmann machine?\n",
    "\n",
    "When we sample, how often do we see the various possible states? To answer this question, let's look at the sample matrix `S`. Each column represents the state of the network at a particular turn. We can count how many times this low energy state appears in our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71e5695-2478-483b-bac8-66110bb492a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×10000 Matrix{Int64}:\n",
       "  1  1  -1  -1  -1  -1  -1  1  1   1  -1  …  1   1   1  1  -1  -1  -1  -1   1\n",
       " -1  1  -1  -1   1   1  -1  1  1  -1  -1     1  -1  -1  1  -1  -1   1  -1  -1\n",
       "  1  1   1  -1   1   1   1  1  1  -1   1     1   1   1  1  -1   1  -1  -1  -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3c249",
   "metadata": {},
   "source": [
    "What is the frequency of observing each state of the Boltzmann machine in our samples? Let's compute this frequency for all possible states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bdeeaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- --------------- ----- ----------- -----------\n",
      " \u001b[1m state \u001b[0m \u001b[1m configuration \u001b[0m \u001b[1m   β \u001b[0m \u001b[1m    energy \u001b[0m \u001b[1m frequency \u001b[0m\n",
      " ------- --------------- ----- ----------- -----------\n",
      "      0    [-1, -1, -1]   0.1   -0.373756      0.1417\n",
      "      1     [-1, -1, 1]   0.1    -3.51589      0.1682\n",
      "      2     [-1, 1, -1]   0.1   -0.580852      0.1104\n",
      "      3      [-1, 1, 1]   0.1    0.861006       0.129\n",
      "      4     [1, -1, -1]   0.1     1.79611      0.1125\n",
      "      5      [1, -1, 1]   0.1    -3.21171      0.1375\n",
      "      6      [1, 1, -1]   0.1     2.72446      0.0956\n",
      "      7       [1, 1, 1]   0.1     2.30063      0.1051\n",
      " ------- --------------- ----- ----------- -----------\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "\n",
    "    N = 2^number_of_nodes; # number of possible states\n",
    "    counts = zeros(Int, N); # visit counts per state\n",
    "    rows = NamedTuple[];\n",
    "\n",
    "    for j ∈ 1:size(S, 2)\n",
    "        state = Int.(S[:, j]);\n",
    "        idx = sum(div.(reverse(state) .+ 1, 2) .* (2 .^ (0:(number_of_nodes - 1))));\n",
    "        counts[idx + 1] += 1;\n",
    "    end\n",
    "\n",
    "    # compute the table entries -\n",
    "    frequencies = counts ./ size(S, 2);\n",
    "    for i ∈ 0:(N - 1)\n",
    "        sᵢ = digits(i, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse; # map index to state vector\n",
    "        push!(rows, (\n",
    "            state = i,\n",
    "            configuration = sᵢ,\n",
    "            β = β,\n",
    "            energy = energy_state_array[i + 1],\n",
    "            frequency = frequencies[i + 1],\n",
    "        ));\n",
    "    end\n",
    "\n",
    "    # show the table -\n",
    "    pretty_table(\n",
    "        rows;\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact),\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab557fa-f62d-42bc-a8cc-f98696c1ba9a",
   "metadata": {},
   "source": [
    "### Things to think about\n",
    "* __Question__: In the sample code block above, we have used a particular heuristic to select the initial state for the sampling. What are alternative approaches to initialize the sampling? Consider at least two different methods and their consequences.\n",
    "* __Question__: How would you expect the frequency of observing a state to change if we increased or decreased the value of $\\beta$? Why?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f2ed7",
   "metadata": {},
   "source": [
    "## Task 2: What is the stationary distribution?\n",
    "In this task, let's explore the stationary distribution of the Boltzmann machine. Since this Boltzmann machine is small (three nodes), we can compute the stationary distribution exactly. We'll compare this exact stationary distribution to the empirical stationary distribution that we estimate from the samples generated in the previous section.\n",
    "\n",
    "### Theory\n",
    "After a _sufficiently large_ number of turns, the network configurations (state vectors) $\\mathbf{s}^{(1)},\\mathbf{s}^{(2)},\\dots,$ of the Boltzmann Machine will converge to a _stationary distribution_ over the state configurations $\\mathbf{s}\\in\\mathcal{S}$ which can be modeled as [a Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution) of the form:\n",
    "$$\n",
    "P(\\mathbf{s}) = \\frac{1}{Z(\\mathcal{S},\\beta)}\\exp\\left(-\\beta\\cdot{E(\\mathbf{s})}\\right)\n",
    "$$\n",
    "where $E(\\mathbf{s})$ is the energy of state $\\mathbf{s}$, the $\\beta$ is the (inverse) temperature of the system, and $Z(\\mathcal{S},\\beta)$ is the partition function. The energy of configuration $\\mathbf{s}\\in\\mathcal{S}$ is given by:\n",
    "$$\n",
    "E(\\mathbf{s}) = -\\sum_{i\\in\\mathcal{V}} b_{i}s_{i} - \\frac{1}{2}\\sum_{i,j\\in\\mathcal{V}} w_{ij}s_{i}s_{j}\n",
    "$$\n",
    "where the first term is the energy associated with the bias terms, and the second term is the energy associated with the weights of the connections. The partition function $Z(\\mathcal{S},\\beta)$ is difficult to compute in practice; however, it is given by:\n",
    "$$\n",
    "Z(\\mathcal{S},\\beta) = \\sum_{\\mathbf{s}^{\\prime}\\in\\mathcal{S}}\\exp\\left({-\\beta\\cdot{E}(\\mathbf{s}^{\\prime})}\\right)\n",
    "$$\n",
    "where $\\mathcal{S}$ is the set of _all possible network configurations_ of the Boltzmann Machine. \n",
    "\n",
    "> __Note__: The partition function $Z(\\mathcal{S},\\beta)$ is a normalizing constant that ensures that the probabilities sum to 1. However, for even a moderately sized system, the partition function is impossible to compute because it involves summing over all possible network configurations, which grows exponentially with the number of nodes. For example, in our case, that is $2^{n}$, where $n$ is the number of nodes in the network. For our simple three-node Boltzmann machine, the partition function will sum $2^{3} = 8$ states. \n",
    "\n",
    "Let's enumerate these $2^3 = 8$ states using [the `digits(...)` method](https://docs.julialang.org/en/v1/base/math/#Base.ndigits) and compute the partition function. The code below converts each integer $i \\in \\{0,1,\\ldots,7\\}$ to its binary representation and then maps it to the $\\{-1,1\\}$ encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2840cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z,configurations = let\n",
    "\n",
    "    # initialize -\n",
    "    Z = Dict{Int,Float64}();\n",
    "    configurations = Dict{Int,Vector{Int}}();\n",
    "    N = 2^number_of_nodes; # how many configurations do we have\n",
    "\n",
    "    # loop throught each configuration\n",
    "    for i ∈ 0:(N - 1)\n",
    "        sᵢ = digits(i, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse # convert integer to binary state vector\n",
    "        Z[i] = exp(-2*β*energy(model, sᵢ)); # calculate the partition function\n",
    "        configurations[i] = sᵢ; # store the configuration\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    Z,configurations\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9572fc6",
   "metadata": {},
   "source": [
    "### Compute the _actual_ stationary distribution\n",
    "\n",
    "Let's compute the stationary distribution of the Boltzmann Machine using the Boltzmann distribution. We'll compute the energy of each state configuration $\\mathbf{s}\\in\\mathcal{S}$ and then compute the probability of each state configuration using the Boltzmann distribution. \n",
    "\n",
    "We'll save the probabilities in the `P::Dict{Int,Array{Float64,1}}` dictionary where the key is the state configuration and the value is the probability of the state configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c791291",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = let\n",
    "    \n",
    "    # initialize -\n",
    "    P = Dict{Int,Float64}();\n",
    "    N = 2^number_of_nodes; # how many configurations do we have\n",
    "\n",
    "    # what is the normalizing constant\n",
    "    Z̄ = sum(values(Z)); # calculate the value of the partition function\n",
    "\n",
    "    # loop through each configuration\n",
    "    for i ∈ 0:(N - 1)\n",
    "        P[i] = Z[i]/Z̄; # calculate the probability of each configuration\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d080054",
   "metadata": {},
   "source": [
    "__Check__: Does the _actual_ stationary Boltzmann distribution sum to `1` (use [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert)? If not, then we have a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4eca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert sum(values(P)) ≈ 1.0 # if this fails: we get an AssertionError, otherwise nothing happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eaa05c",
   "metadata": {},
   "source": [
    "__Estimate the empirical stationary distribution__: Next, compute the empirical estimate of the stationary distribution by analyzing the simulation samples. If we generate enough samples, the empirical distribution should be similar to the stationary distribution. \n",
    "> __Idea__: Compute the number of times a particular configuration $\\mathbf{s}\\in\\mathcal{S}$ occurs in the simulation sample matrix $\\mathbf{S}$ for each of the configurations, and then divide by the total number of samples to get the probability of each configuration. This gives us the _empirical_ distribution of the samples.\n",
    "\n",
    "We'll save the empirical probabilities in the `P̂::Dict{Int, Array{Float64,1}}` dictionary, where the key is the state configuration index and the value is the empirical probability of that state configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cfeaa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "P̂ = let\n",
    "   \n",
    "    # initialize -\n",
    "    P̂ = Dict{Int,Float64}();\n",
    "    N = 2^number_of_nodes; # how many configurations do we have\n",
    "    number_of_turns = size(S,2); # how many turns do we have\n",
    "\n",
    "    for i ∈ 0:(N - 1)\n",
    "        sᵢ = digits(i, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1 |> reverse # count by base 2, and convert to -1,1\n",
    "\n",
    "        counter = 0;\n",
    "        for j ∈ 1:number_of_turns\n",
    "            if (S[:,j] == sᵢ)\n",
    "                counter += 1;\n",
    "            end\n",
    "        end\n",
    "        P̂[i] = counter/number_of_turns;\n",
    "    end\n",
    "    \n",
    "    P̂\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195139b0",
   "metadata": {},
   "source": [
    "__Check__: Does the empirical stationary Boltzmann distribution sum to `1` (use [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert))? If not, then we have a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86636f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert sum(values(P̂)) ≈ 1.0 # if this fails: we get an AssertionError, otherwise nothing happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb973a",
   "metadata": {},
   "source": [
    "Unhide the code block below to see how we constructed the probability table for the actual and empirical stationary distributions.\n",
    "\n",
    "> __What do we expect?__ If we generate enough samples, the empirical distribution $\\hat{P}$ should converge to the actual Boltzmann distribution $P$. The ranking of states by probability should match between the two distributions.\n",
    "\n",
    "Do we see what we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f5f793",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- --------------- ----------- --------- ----------- --------- ------- -------\n",
      " \u001b[1m     i \u001b[0m \u001b[1m configuration \u001b[0m \u001b[1m    energy \u001b[0m \u001b[1m       β \u001b[0m \u001b[1m         P \u001b[0m \u001b[1m       P̂ \u001b[0m \u001b[1m     r \u001b[0m \u001b[1m     r̂ \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m Vector{Int64} \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m\n",
      " ------- --------------- ----------- --------- ----------- --------- ------- -------\n",
      "      0    [-1, -1, -1]   -0.373756       0.1    0.121448    0.1417       4       2\n",
      "      1     [-1, -1, 1]    -3.51589       0.1    0.227675    0.1682       1       1\n",
      "      2     [-1, 1, -1]   -0.580852       0.1    0.126584    0.1104       3       6\n",
      "      3      [-1, 1, 1]    0.861006       0.1   0.0948729     0.129       5       4\n",
      "      4     [1, -1, -1]     1.79611       0.1     0.07869    0.1125       6       5\n",
      "      5      [1, -1, 1]    -3.21171       0.1    0.214237    0.1375       2       3\n",
      "      6      [1, 1, -1]     2.72446       0.1   0.0653558    0.0956       8       8\n",
      "      7       [1, 1, 1]     2.30063       0.1   0.0711373    0.1051       7       7\n",
      " ------- --------------- ----------- --------- ----------- --------- ------- -------\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "   \n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    N = 2^number_of_nodes; # how many configurations do we have\n",
    "\n",
    "    # compute the ordinal rank -\n",
    "    r = [P[i] for i ∈ 0:(N - 1)] |> x -> ordinalrank(x, rev = true);\n",
    "    r̂ = [P̂[i] for i ∈ 0:(N - 1)] |> x -> ordinalrank(x, rev = true);\n",
    "\n",
    "    # main -\n",
    "    for i ∈ 0:(N - 1)\n",
    "        sᵢ = digits(i, base = 2, pad = number_of_nodes) |> x -> 2*x .- 1|> reverse # count by base 2, and convert to -1,1\n",
    "        row_df = (\n",
    "            i = i,\n",
    "            configuration = sᵢ,\n",
    "            energy = energy(model, sᵢ),\n",
    "            β = β,\n",
    "            P = P[i],\n",
    "            P̂ = P̂[i],\n",
    "            r = r[i+1],\n",
    "            r̂ = r̂[i+1],\n",
    "        )\n",
    "        push!(df, row_df)\n",
    "    end\n",
    "    \n",
    "    pretty_table(\n",
    "        df;\n",
    "        fit_table_in_display_horizontally = false,\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact),\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71505a-b98b-4d4f-872d-875b7a9c7612",
   "metadata": {},
   "source": [
    "### Things to think about\n",
    "* __Question__: What happens to the probability of the states as we change the system (inverse) temperature $\\beta$, i.e., do we see different behavior for (cool) $\\beta\\gg{1}$ versus (hot) $\\beta\\ll{1}$ systems?\n",
    "* __Question__: In the gradient ascent training algorithm, the new step is given by: $\\Delta{w_{ij}} = \\eta\\left(\\langle{x_{i}x_{j}}\\rangle_{\\mathbf{X}} - \\langle{s_{i}s_{j}}\\rangle_{\\mathbf{S}}\\right)$. What is your interpretation of this, and how would we compute this update?\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2889b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This lab explored sampling and stationary distributions in a small Boltzmann machine.\n",
    "\n",
    "> __Key Takeaways__\n",
    ">\n",
    "> * **Boltzmann distribution:** After sufficient sampling, the state configurations of a Boltzmann machine converge to a Boltzmann distribution where lower-energy states have higher probability.\n",
    "> * **Partition function limitation:** Computing the exact stationary distribution requires summing over all $2^n$ configurations, which is intractable for large networks.\n",
    "> * **Training challenge:** Each training iteration requires sampling until the network reaches its stationary distribution, making training computationally expensive.\n",
    "\n",
    "The need to reach stationarity at each training step motivates the development of restricted Boltzmann machines and contrastive divergence.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.2",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
