{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L12a: Introduction to Recurrent Neural Networks (RNNs)\n",
    "In this lecture, we will explore the fundamentals of Recurrent Neural Networks (RNNs), a class of neural networks designed to handle sequential data. RNNs are particularly useful for tasks such as language modeling, time series prediction, and sequence classification.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> \n",
    "> Three learning objectives for this lecture.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc6854",
   "metadata": {},
   "source": [
    "## Example\n",
    "Today, we will use the following examples to illustrate key concepts:\n",
    " \n",
    "> [▶ Can we estimate the similarity of different firms?](CHEME-5820-L4a-Example-MeasureFirmSimilarityScores-Spring-2026.ipynb). In this example, let's explore how to measure the similarity between different firms based upon the similarity of their daily growth rates over 10-year periods. Does this similarity correlate with other firm metrics, e.g., business sector, market capitalization, etc.?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ee357",
   "metadata": {},
   "source": [
    "## General problem: Modeling a Sequence\n",
    "Suppose we have a _sequence of data_ $(x_1, x_2, \\ldots, x_T)$ where $T$ is the sequence length, and $x_i$ is the $i$-th element (token) of the sequence. What are some examples of sequences?\n",
    "\n",
    "> __Examples__\n",
    "> \n",
    "> In natural language processing, $x_{i}$ could be words or characters in a text. On the other hand, in time series analysis, $x_t$ could be a measurement, i.e., temperature, pressure, price, etc, at time $t$.\n",
    "\n",
    "To model a sequence, i.e., predict the next token given past tokens, we _could try_ to use tools such as [Hidden Markov Models (HMMs)](https://en.wikipedia.org/wiki/Hidden_Markov_model). However, HMMs are limited in their ability to capture _long-range dependencies_ and complex relationships between elements in the sequence. \n",
    "\n",
    "> __Why not HMMs?__\n",
    ">\n",
    "> Hidden Markov Models (HMMs) use the [Markov property](https://en.wikipedia.org/wiki/Markov_property), which says that the future state of a system depends only on its current state and not on its past states. This assumption is often too restrictive for many real-world applications, where the relationships between elements in a sequence can be more complex and require a more flexible modeling approach.\n",
    "\n",
    "This is where RNNs come in. RNNs are designed to handle data sequences by maintaining a hidden state that captures information about previous inputs. This allows them to model long-range dependencies and contextual relationships between elements in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbd283",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <center>\n",
    "      <img\n",
    "        src=\"figs/Recurrent_neural_network_unfold.svg\"\n",
    "        alt=\"General Tree Example\"\n",
    "        height=\"400\"\n",
    "        width=\"800\"\n",
    "      />\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c1c19",
   "metadata": {},
   "source": [
    "## What are Recurrent Neural Networks (RNNs)?\n",
    "Recurrent Neural Networks (RNNs) are artificial neural networks designed to process sequential data by retaining information about previous inputs through their internal memory. \n",
    "\n",
    "> __How are RNNs different from feedforward neural networks?__\n",
    ">\n",
    "> * __Do feedforward neural networks have memory?__ No, feedforward neural networks process do not retain information about previous inputs. Thus, the parameters (weights and bias values) do not change once training is over. This means that the network is done learning and evolving. When we feed in values, an FNN applies the operations that make up the network using the values it has learned.\n",
    "> * __How are RNNs different from feedforward neural networks?__ RNNs have connections that loop back on themselves, allowing them to maintain a _hidden state_ that captures information about previous inputs. This makes RNNs particularly effective for tasks such as language modeling, time-series prediction, and speech recognition, where context and dependencies between data points are crucial. \n",
    "\n",
    "Let's look at two types of _simple_ RNNs: the Elman and Jordan networks.\n",
    "\n",
    "### Elman Network: Mathematical Formulation\n",
    "The Elman network is a simple RNN type consisting of an input layer, a hidden layer, and an output layer. The hidden layer has recurrent connections that allow it to maintain a hidden state over time: [Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2), 179-211.](https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1)\n",
    "\n",
    "__At each time step__: an Elman RNN takes an _input_ and the previous hidden state (memory) and computes the output entry at time $t$.  Let the input vector at time $t$ be denoted as $\\mathbf{x}_t\\in\\mathbb{R}^{d_{in}}$, the hidden state at time $t$ as $\\mathbf{h}_t\\in\\mathbb{R}^{h}$, and the output at time $t$ as $\\mathbf{y}_t\\in\\mathbb{R}^{d_{out}}$. \n",
    "\n",
    "> __Elman RNN Architecture__\n",
    ">\n",
    "> The following equations can describe the Elman RNN:\n",
    "> $$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_t &= \\sigma_{h}(\\mathbf{U}_h \\mathbf{h}_{t-1} + \\mathbf{W}_x \\mathbf{x}_t + \\mathbf{b}_h) \\\\\n",
    "\\mathbf{y}_t &= \\sigma_{y}(\\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y)\n",
    "\\end{align*}}\n",
    "> $$\n",
    "> where the parameters are:\n",
    "> * __Network weights__: the term $\\mathbf{U}_h\\in\\mathbb{R}^{h\\times{h}}$ is the weight matrix for the hidden state, $\\mathbf{W}_x\\in\\mathbb{R}^{h\\times{d_{in}}}$ is the weight matrix for the input, and $\\mathbf{W}_y\\in\\mathbb{R}^{d_{out}\\times{h}}$ is the weight matrix for the output\n",
    "> * __Network bias__: the $\\mathbf{b}_h\\in\\mathbb{R}^{h}$ terms denote the bias vector for the hidden state, and $\\mathbf{b}_y\\in\\mathbb{R}^{d_{out}}$ is the bias vector for the output.\n",
    "> * __Activation function__: the $\\sigma_{h}$ function is a _hidden layer activation function_, such as the sigmoid or hyperbolic tangent (tanh) function, which introduces non-linearity into the RNN. The activation function $\\sigma_{y}$ is an _output activation function_ that can be a softmax function for classification tasks or a linear function for regression tasks.\n",
    "\n",
    "How many parameters are there in the Elman network? The number of parameters in an Elman RNN can be calculated as follows:\n",
    "* _Hidden state_: The number of parameters for the hidden state is $h^2 + d_{in}h + h = h(h + d_{in} + 1)$\n",
    "* _Output_: The number of parameters for the output is $d_{out}h + d_{out} = d_{out}(h + 1)$\n",
    "* _Total_: The total number of parameters in the Elman RNN is $h(h + d_{in} + 1) + d_{out}(h + 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1e87e",
   "metadata": {},
   "source": [
    "### Jordan Network: Mathematical Formulation\n",
    "The Jordan network is another type of RNN similar to the Elman network but with a different architecture. In a Jordan network, the output layer is connected back to the hidden layer, allowing the network to maintain a hidden state based on the output at the previous time step.\n",
    "* [Jordan, Michael I. (1997-01-01). \"Serial Order: A Parallel Distributed Processing Approach\". Neural-Network Models of Cognition — Biobehavioral Foundations. Advances in Psychology. Vol. 121. pp. 471–495. doi:10.1016/s0166-4115(97)80111-2. ISBN 978-0-444-81931-4. S2CID 15375627.](https://www.sciencedirect.com/science/article/pii/S0166411597801112?via%3Dihub)\n",
    "\n",
    "__At each time step__: a Jordan RNN takes an _input_, the previous hidden state (memory), and the previous output and computes the output entry at time $t$. Thus, the Jordan network has a similar structure to the Elman network but with a different way of maintaining the hidden state (i.e., the output layer is connected back to the hidden layer).\n",
    "\n",
    "Let the input vector at time $t$ be denoted as $\\mathbf{x}_t\\in\\mathbb{R}^{d_{in}}$, the hidden state at time $t$ as $\\mathbf{h}_t\\in\\mathbb{R}^{h}$, \n",
    "the state vector at time $t$\n",
    "\n",
    "> __Jordan RNN Architecture__\n",
    "> \n",
    "> The Jordan RNN can be described by the following equations:\n",
    "> $$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_t &= \\sigma_{h}(\\mathbf{U}_h \\mathbf{s}_{t} + \\mathbf{W}_h \\mathbf{x}_t + \\mathbf{b}_h) \\\\\n",
    "\\mathbf{y}_t &= \\sigma_{y}(\\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y) \\\\\n",
    "\\mathbf{s}_t &= \\sigma_{s}(\\mathbf{W}_{ss} \\mathbf{s}_{t-1} + \\mathbf{W}_{sy} \\mathbf{y}_{t-1} + \\mathbf{b}_s) \\\\\n",
    "\\end{align*}}\n",
    "> $$\n",
    "> where the parameters are:\n",
    "> * __Network weights__: the term $\\mathbf{U}_h\\in\\mathbb{R}^{h\\times{s}}$ is the weight matrix for the hidden state with respect to $s$, $\\mathbf{W}_h\\in\\mathbb{R}^{h\\times{d_{in}}}$ is the weight matrix for the input, and $\\mathbf{W}_y\\in\\mathbb{R}^{d_{out}\\times{h}}$ is the weight matrix for the output. In addition, a Jordan network has parameters associated with the state $\\mathbf{s}$, the $\\mathbf{W}_{ss}\\in\\mathbb{R}^{h\\times{s}}$ matrix is the weight matrix for the state with respect to the previous $s$, and $\\mathbf{W}_{sy}\\in\\mathbb{R}^{h\\times{d_{out}}}$ is the weight matrix for the state with respect to the previous $y$.\n",
    "> * __Network bias__: the $\\mathbf{b}_h\\in\\mathbb{R}^{h}$ terms denotes the bias vector for the hidden state, $\\mathbf{b}_y\\in\\mathbb{R}^{d_{out}}$ is the bias vector for the output and $\\mathbf{b}_s\\in\\mathbb{R}^{h}$ is the bias vector for the state.\n",
    "> * __Activation function__: the $\\sigma_{h}$ function is a _hidden layer activation function_, such as the sigmoid or hyperbolic tangent (tanh) function, which introduces non-linearity into the RNN. The activation function $\\sigma_{y}$ is an _output activation function_ that can be a softmax function for classification tasks or a linear function for regression tasks, and $\\sigma_{s}$ is a _state activation function_ that can be a sigmoid or tanh function.\n",
    "\n",
    "How many parameters are there in the Jordan network? \n",
    "\n",
    "> __Parameter Count in Jordan RNN__\n",
    ">\n",
    "> The number of parameters in a Jordan RNN can be calculated as follows:\n",
    "> * _Hidden state_: The number of parameters for the hidden state is $N_{hidden} = sh + d_{in}h + h = h(s + d_{in} + 1)$\n",
    "> * _Output_: The number of parameters for the output is $N_{output} = d_{out}h + d_{out} = d_{out}(h + 1)$\n",
    "> * _State_: The number of parameters for the state is $N_{state} = s^2 + sd_{out} + s = s(s + d_{out} + 1)$\n",
    ">\n",
    "> The total number of parameters in the Jordan RNN is given by: \n",
    "> $$\n",
    "\\begin{align*}\n",
    "N_{p} &= N_{hidden} + N_{output} + N_{state} \\\\\n",
    "N_{p} & = h(s + d_{in} + 1) + d_{out}(h + 1) + s(s + d_{out} + 1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002bc33",
   "metadata": {},
   "source": [
    "## Training challenges with RNNs\n",
    "The training process for RNNs is similar to that of feedforward neural networks but with a few key differences. The main difference is that RNNs are trained using _backpropagation through time_ (BPTT), which _unrolls the network_ across sequential steps to compute gradients and update shared weights. \n",
    "\n",
    "> __Backpropagation through time (BPTT)__\n",
    ">\n",
    "> * __What is BPTT?__ Backpropagation through time (BPTT) is a variant of the backpropagation algorithm that trains recurrent neural networks (RNNs). It involves _unrolling_ the RNN across time steps, treating it as a feedforward network, and then applying the standard backpropagation algorithm to compute gradients and update weights. BPTT allows RNNs to learn from data sequences by capturing temporal dependencies and adjusting weights based on the entire sequence.\n",
    "> * __Issues__: However, BPTT is prone to the __vanishing gradients problem__, where gradients shrink exponentially during backpropagation, hindering the learning of long-term dependencies, and the __exploding gradients problem__, where unchecked gradient growth destabilizes training. \n",
    "\n",
    "__Hmmm__: Suppose we didn't use gradient descent but instead used a different optimization algorithm, such as genetic algorithms, simulated annealing, or particle swarm optimization. Would that help with the vanishing gradients problem?\n",
    "\n",
    "For more information (and intuition) about BPTT and the vanishing and exploding gradients problem, see [Chapter 10 of Goodfellow et al.](http://www.deeplearningbook.org/). These training challenges (and other factors) led to advanced architectures like [Long short-term memory (LSTMs) and Gated Recurrent Units (GRUs)](https://arxiv.org/pdf/1412.3555), which use gating mechanisms to better regulate information flow and mitigate gradient issues.\n",
    "\n",
    "> __What is gating?__ Gating mechanisms are components in neural networks, particularly in recurrent neural networks (RNNs), that control the flow of information by selectively allowing or blocking specific inputs or activations. They help manage the network's memory and learning process, enabling it to retain relevant information over time and discard irrelevant data. \n",
    "\n",
    "Let's watch [a Video from the IBM technology channel about LSTMs](https://www.yout-ube.com/watch?v=b61DPVFX03I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75017fa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One summary sentence about the lecture goes here.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> Three key takeaways from the lecture go here.\n",
    "\n",
    "One concluding sentence about the lecture goes here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
