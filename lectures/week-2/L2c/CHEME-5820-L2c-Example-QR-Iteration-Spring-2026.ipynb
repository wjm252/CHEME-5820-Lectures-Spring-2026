{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8771d2db",
   "metadata": {},
   "source": [
    "# Example: Computing the Eigendecomposition using QR decomposition of a Covariance Matrix\n",
    "In this example, we will compute the eigendecomposition of a covariance matrix using the QR algorithm, which relies on the Gram-Schmidt process for orthogonalization. The covariance matrix will be computed from the daily log growth rate of stock prices.\n",
    "\n",
    "> __Learning Objectives__\n",
    "> \n",
    "> By the end of this example, you should be able to:\n",
    "> THree learning objectives go here\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772fb80",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345c2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5328f7",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d12776",
   "metadata": {},
   "source": [
    "### Data\n",
    "We gathered a daily open-high-low-close dataset for each firm in the S&P 500 from `01-03-2014` until `12-31-2024`, along with data for a few exchange-traded funds and volatility products during that time. \n",
    "\n",
    "Let's load the `original_dataset::DataFrame` by calling [the `MyTrainingMarketDataSet()` function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyTrainingMarketDataSet) and remove firms that do not have the maximum number of trading days. The cleaned dataset $\\mathcal{D}$ will be stored in the `dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeaafdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = MyTrainingMarketDataSet() |> x-> x[\"dataset\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72747488",
   "metadata": {},
   "source": [
    "Not all tickers in our dataset have the maximum number of trading days for various reasons, e.g., acquisition or de-listing events. Let's collect only those tickers with the maximum number of trading days.\n",
    "\n",
    "First, let's compute the number of records for a firm that we know has a maximum value, e.g., `AAPL`, and save that value in the `maximum_number_trading_days::Int64` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36621b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_number_trading_days = original_dataset[\"AAPL\"] |> nrow;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157167e",
   "metadata": {},
   "source": [
    "Now, let's iterate through our data and collect only tickers with `maximum_number_trading_days` records. Save that data in the `dataset::Dict{String,DataFrame}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d73324cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = let\n",
    "\n",
    "    dataset = Dict{String,DataFrame}();\n",
    "    for (ticker,data) ∈ original_dataset\n",
    "        if (nrow(data) == maximum_number_trading_days)\n",
    "            dataset[ticker] = data;\n",
    "        end\n",
    "    end\n",
    "    dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b1fde",
   "metadata": {},
   "source": [
    "Finally, let's get a list of the firms in our cleaned dataset (and sort them alphabetically). We store the sorted firm ticker symbols in the `list_of_tickers::Array{String,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ffa2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424-element Vector{String}:\n",
       " \"A\"\n",
       " \"AAL\"\n",
       " \"AAP\"\n",
       " \"AAPL\"\n",
       " \"ABBV\"\n",
       " \"ABT\"\n",
       " \"ACN\"\n",
       " \"ADBE\"\n",
       " \"ADI\"\n",
       " \"ADM\"\n",
       " ⋮\n",
       " \"WYNN\"\n",
       " \"XEL\"\n",
       " \"XOM\"\n",
       " \"XRAY\"\n",
       " \"XYL\"\n",
       " \"YUM\"\n",
       " \"ZBRA\"\n",
       " \"ZION\"\n",
       " \"ZTS\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_tickers = keys(dataset) |> collect |> sort # list of firm \"ticker\" symbols in alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75618a1f",
   "metadata": {},
   "source": [
    "Finally, let's set up a ticker map that holds the index of each ticker value. We'll save this in the `tickerindexmap::Dict{String,Int}` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ead51ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Int64} with 424 entries:\n",
       "  \"EMR\"  => 132\n",
       "  \"CTAS\" => 101\n",
       "  \"HSIC\" => 187\n",
       "  \"KIM\"  => 217\n",
       "  \"PLD\"  => 310\n",
       "  \"IEX\"  => 194\n",
       "  \"BAC\"  => 48\n",
       "  \"CBOE\" => 69\n",
       "  \"EXR\"  => 144\n",
       "  \"NCLH\" => 271\n",
       "  \"CVS\"  => 103\n",
       "  \"DRI\"  => 119\n",
       "  \"DTE\"  => 120\n",
       "  \"ZION\" => 423\n",
       "  \"AVY\"  => 43\n",
       "  \"EW\"   => 140\n",
       "  \"EA\"   => 124\n",
       "  \"NWSA\" => 289\n",
       "  \"CAG\"  => 65\n",
       "  ⋮      => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tickerindexmap = let\n",
    "\n",
    "    # initialize -\n",
    "    tickerindexmap = Dict{String,Int}();\n",
    "    for i ∈ eachindex(list_of_tickers)\n",
    "        tickerindexmap[list_of_tickers[i]] = i;\n",
    "    end\n",
    "\n",
    "    tickerindexmap;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eb5f1",
   "metadata": {},
   "source": [
    "### Compute the return matrix\n",
    "Next, let's compute the return array which contains, for each day and each firm in our dataset, the value of the growth rate between time $j$ and $j-1$. \n",
    "\n",
    ">  __Continuously Compounded Growth Rate (CCGR)__\n",
    ">\n",
    "> Let's assume a model of the share price of firm $i$ is governed by an expression of the form:\n",
    ">$$\n",
    "\\begin{align*}\n",
    "S^{(i)}_{j} &= S^{(i)}_{j-1}\\;\\exp\\left(g^{(i)}_{j,j-1}\\Delta{t}_{j}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "> where $S^{(i)}_{j-1}$ denotes the share price of firm $i$ at time index $j-1$, $S^{(i)}_{j}$ denotes the share price of firm $i$ at time index $j$, and $\\Delta{t}_{j} = t_{j} - t_{j-1}$ denotes the length of a time step (units: years) between time index $j-1$ and $j$. The value we are going to estimate is the growth rate $g^{(i)}_{j,j-1}$ (units: inverse years) for each firm $i$, and each time step in the dataset.\n",
    "\n",
    "We've implemented [the `log_growth_matrix(...)` function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.log_growth_matrix) which takes the cleaned dataset and a list of ticker symbols, and returns the growth rate array. Each row of the growth rate array is a time step, while each column corresponds to a firm from the `list_of_tickers::Array{String,1}` array.\n",
    "\n",
    "We save the growth rate array in the `X::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea1cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = let\n",
    "\n",
    "    # initialize -\n",
    "    r̄ = 0.0; # assume the risk-free rate is 0\n",
    "\n",
    "    # compute the growth matrix -\n",
    "    growth_rate_array = log_growth_matrix(dataset, list_of_tickers, Δt = 1.0, \n",
    "        risk_free_rate = r̄); # other optional parameters are at their defaults\n",
    "\n",
    "    growth_rate_array; # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e3fbc",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e901e4",
   "metadata": {},
   "source": [
    "## Task 1: Compute the Empirical Covariance Matrix\n",
    "In this task, let's compute the empirical covariance matrix $\\hat{\\mathbf{\\Sigma}}$ for our dataset $\\mathcal{D}$ using code that we write ourselves (we'll never do this in practice, but it's a good exercise). The empirical covariance matrix is given by:\n",
    "$$\n",
    "\\hat{\\mathbf{\\Sigma}} = \\frac{1}{n-1}\\tilde{\\mathbf{X}}^{\\top}\\tilde{\\mathbf{X}}\n",
    "$$\n",
    "where $\\tilde{\\mathbf{X}}$ is the centered data matrix:\n",
    "$$\n",
    "\\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1}\\mathbf{m}^{\\top}\n",
    "$$\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a vector of ones, and $\\mathbf{1}\\mathbf{m}^{\\top}$ creates an $n \\times m$ matrix where each row is identical and contains the __returns__ on the columns. \n",
    "\n",
    "> __Outer product:__ The $\\mathbf{1}\\mathbf{m}^{\\top}$ is an example of an outer product. The [outer product](https://en.wikipedia.org/wiki/Outer_product) of two vectors $\\mathbf{a} \\in \\mathbb{R}^{n}$ and $\\mathbf{b} \\in \\mathbb{R}^{m}$ is the $n \\times m$ matrix $\\mathbf{a}\\mathbf{b}^{\\top}$. Each element of the outer product is computed as $(\\mathbf{a}\\mathbf{b}^{\\top})_{ij} = a_i b_j$. \n",
    "\n",
    "Let's start by constructing the data matrix $\\mathbf{X} \\in\\mathbb{R}^{n \\times m}$ where each row $k$ contains the __returns__ for all $m$ firms at time period $k$. To compute the returns, we [use the `log_growth_matrix(...)` function from the `VLQuantitativeFinancePackage.jl` package](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/equity/#VLQuantitativeFinancePackage.log_growth_matrix) and multiply by the time step $\\Delta{t}$. \n",
    "\n",
    "First, let's compute the mean returns for each firm and store them in the `m::Array{Float64,1}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8832dbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424-element Vector{Float64}:\n",
       "  0.00043152690397797527\n",
       " -0.0001469541078208755\n",
       " -0.00031710102271930243\n",
       "  0.0009238140812477853\n",
       "  0.00044105742821765466\n",
       "  0.00038868965269282677\n",
       "  0.0005289761752874106\n",
       "  0.0007284547060446172\n",
       "  0.0005265024709424277\n",
       "  5.618535648601576e-5\n",
       "  ⋮\n",
       " -0.00029807586991059844\n",
       "  0.00032461074815436536\n",
       "  2.6916182694995763e-5\n",
       " -0.0003392763144159303\n",
       "  0.00043857557703725285\n",
       "  0.0002064503070884351\n",
       "  0.0007130714183989561\n",
       "  0.00021701413350653834\n",
       "  0.0005858730432623267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = mean(X, dims=1) |> vec # mean returns for each firm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8be80",
   "metadata": {},
   "source": [
    "Now, let's form the centered data matrix $\\tilde{\\mathbf{X}}$ by subtracting the mean returns from each row of the data matrix $\\mathbf{X}$. We store the centered data in the `X_centered::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2873ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2766×424 Matrix{Float64}:\n",
       " 0.000431527  -0.000146954  -0.000317101  …  0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101  …  0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " ⋮                                        ⋱               \n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101  …  0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101     0.000217014  0.000585873\n",
       " 0.000431527  -0.000146954  -0.000317101  …  0.000217014  0.000585873"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r, c = size(X)\n",
    "ones_vector = ones(r)\n",
    "⊗(ones_vector, m) # outer product of ones_vector and m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db621811",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ae0c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2766×424 Matrix{Float64}:\n",
       " -0.00391389    0.0250717    -0.0110757    …   0.000758758  -0.00457502\n",
       "  0.0107441     0.00439892    0.00584244      -0.00340269    0.00332868\n",
       "  0.0127155     0.00354218    0.000338403      0.00450918   -0.0108297\n",
       "  0.00213365    0.0686387     0.00703199       0.0123199    -0.0020471\n",
       "  0.00677517    0.0103835     0.0134887       -0.00882298    0.0168867\n",
       "  0.00200431   -0.0155826    -0.00282885   …  -0.00779546   -0.0129519\n",
       "  0.0109205    -0.00177269    0.0195462       -0.00726801   -0.00490968\n",
       "  0.00769032    0.0041688     0.00788889       0.0174411    -0.00113277\n",
       "  0.00477837    0.00679031    0.000742733     -0.00869703    0.00511985\n",
       "  0.00441036    0.0244706     0.00401781       0.011203     -0.00628531\n",
       "  ⋮                                        ⋱                \n",
       " -0.0177684     0.0154026    -0.0091056       -0.0366419    -0.0162462\n",
       " -0.0103991    -0.0102059    -0.0398453       -0.0282911    -0.02892\n",
       "  0.00835239    0.0166178     0.0291932        0.0147388     0.00229717\n",
       " -0.00175583    0.0134682     0.000868758  …   0.00973847   -0.00616313\n",
       "  0.0101605    -0.00171521    0.00762603       0.00762274    0.00271019\n",
       " -0.000659302   0.0105804     0.0187943        0.00854145    0.0034231\n",
       " -0.00191887    0.00127964   -0.00640011      -0.00881604   -0.00375384\n",
       " -0.00640088    0.0127564     0.0352639       -0.00537752   -0.0124347\n",
       " -0.00166895   -0.000811671   0.0305096    …   0.000438481   2.04287e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_centered = let \n",
    "    r, c = size(X)\n",
    "    ones_vector = ones(r)\n",
    "    X̃ = X .- ⊗(ones_vector, m);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf1e6d",
   "metadata": {},
   "source": [
    "Finally, let's compute the empirical covariance matrix $\\hat{\\mathbf{\\Sigma}}$ and store it in the `Σ̂::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07bb2393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424×424 Matrix{Float64}:\n",
       " 0.0535771   0.032397    0.0212982  …  0.0355662   0.0280938   0.0266666\n",
       " 0.032397    0.207093    0.0431437     0.051429    0.0689041   0.0280481\n",
       " 0.0212982   0.0431437   0.117391      0.0308539   0.037867    0.019717\n",
       " 0.0229702   0.0312627   0.0163174     0.032633    0.0201827   0.022609\n",
       " 0.0179206   0.0176175   0.0158407     0.0157326   0.0168026   0.0185472\n",
       " 0.0244078   0.0195811   0.0145372  …  0.023159    0.0167416   0.0221966\n",
       " 0.025643    0.0334402   0.0218931     0.0313218   0.0280694   0.023824\n",
       " 0.0294242   0.02952     0.0185233     0.0369478   0.0198333   0.0262802\n",
       " 0.0297618   0.0458278   0.0231343     0.0433011   0.0335821   0.0233085\n",
       " 0.0169025   0.0325409   0.0206063     0.0219846   0.0327002   0.0131991\n",
       " ⋮                                  ⋱                          \n",
       " 0.0339199   0.0921199   0.035255   …  0.0503196   0.0581754   0.0308858\n",
       " 0.00888225  0.00814595  0.0118987     0.00616705  0.00592554  0.0112662\n",
       " 0.0161836   0.0376862   0.0190661     0.0236132   0.0382091   0.0130652\n",
       " 0.0228537   0.0393523   0.0235341     0.0238568   0.0346256   0.0210245\n",
       " 0.0269856   0.0447212   0.0230459     0.0329976   0.036848    0.0216326\n",
       " 0.0195557   0.0341756   0.0216532  …  0.0241318   0.0226535   0.0209732\n",
       " 0.0355662   0.051429    0.0308539     0.115065    0.0449828   0.0297714\n",
       " 0.0280938   0.0689041   0.037867      0.0449828   0.120125    0.020424\n",
       " 0.0266666   0.0280481   0.019717      0.0297714   0.020424    0.0499392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Σ̂ = let \n",
    "\n",
    "    # initialize -\n",
    "    T = 252; # number of trading days in a year\n",
    "    (r,c) = size(X_centered)\n",
    "    Σ = (1/(r-1)) * (X_centered' * X_centered)\n",
    "    Σ*T; # return the annualized empirical covariance matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be432e",
   "metadata": {},
   "source": [
    "__Check__: Let's check our covariance matrix against [the `cov(...)` function from the Julia standard library](https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.cov). Compute the covariance matrix using the built-in function and comapre it to your result:\n",
    "\n",
    "> __Test__ We'll compare the two covariance matrices by computing the Frobenius norm of their difference. The Frobenius norm of a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ is defined as:\n",
    "> $$\n",
    "\\|\\mathbf{A}\\|_{F} = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{m} |a_{ij}|^{2}}\n",
    "> $$\n",
    "> where $a_{ij}$ is the element in the $i^{th}$ row and $j^{th}$ column of matrix $\\mathbf{A}$. If the Frobenius norm of the difference between the two covariance matrices is very small (close to zero), it indicates that they are nearly identical, confirming the correctness of our implementation. We'll use the [`@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to enforce this check.\n",
    "\n",
    "So what do we see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b50a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    ϵ = 1e-8; # tolerance for the Frobenius norm comparison\n",
    "    T = 252; # number of trading days in a year\n",
    "    Σ_builtin = cov(X)*T; # annualized empirical covariance matrix using built\n",
    "    Δ = Σ̂ - Σ_builtin;\n",
    "    frobenius_norm = norm(Δ, 2); # p = 2 is the Frobenius norm for a matrix\n",
    "    test = frobenius_norm < ϵ\n",
    "\n",
    "    # if test fails, throw an error -\n",
    "    @assert test \"Covariance matrices do not match within tolerance!\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacea6e3",
   "metadata": {},
   "source": [
    "Ok! So if we get here without an error, our covariance matrix implementation is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386f14c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2bcf2",
   "metadata": {},
   "source": [
    "## Test 2: Compute the Eigendecomposition using the QR Algorithm\n",
    "In this task, we will compute the eigendecomposition of the empirical covariance matrix $\\hat{\\mathbf{\\Sigma}}$ using our implementation of the QR algorithm in [the `qriteration(...)` function in the `Compute.jl` file](../src/Compute.jl).\n",
    "\n",
    "We'll save the eignenvalues in the `λ̂::Array{Float64,1}` variable and the eigenvectors in the `V̂::Array{Float64,2}` variables. The eigenvalues and eigenvectors are sorted in ascending order based on the eigenvalue magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fec894d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.00020091649390589135, 0.0008004082795630774, 0.0008197993962321032, 0.0011683475444739903, 0.0023239405869121253, 0.0024374860556287833, 0.0029043419685604965, 0.0029450466978901834, 0.0030032234323237325, 0.003141200093345577  …  0.40109541051295494, 0.42410094575995183, 0.4433158835074257, 0.5110538829534128, 0.7218934238657446, 0.8821003299986302, 0.9913463704158854, 1.1983172323103428, 1.595819459020517, 11.438058870655444], [0.002294131234357588 -0.003749383941944555 … 0.05785052459201625 -0.042413850294204546; -0.0009776709554531712 0.000697604059543025 … -0.05786956337900864 -0.08444908198854581; … ; 0.00809290103175616 0.001978653491762944 … -0.07600622038332525 -0.06871769022820234; 0.0019203225901283023 -0.006274549177533917 … 0.06134369996581131 -0.037098632606339624])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(λ̂,V̂) = let\n",
    "\n",
    "    # initilize -\n",
    "    maxiter = 1000; # max number of iterations\n",
    "    tolerance = 1e-9; # tolerance for convergence\n",
    "\n",
    "    # call our QR iteration function -\n",
    "    result = qriteration(Σ̂; maxiter = maxiter, tolerance = tolerance);\n",
    "\n",
    "    λ = result[1]; # eigenvalues\n",
    "    tmpdict = result[2]; # eigenvectors\n",
    "    number_of_rows = length(λ);\n",
    "    V = zeros(number_of_rows, number_of_rows);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        V[:,i] = tmpdict[i];\n",
    "    end\n",
    "\n",
    "    (λ,V); # return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be6b729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = findall(x-> isnan(x) == true, V̂)\n",
    "V̂[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ef329",
   "metadata": {},
   "source": [
    "Before we think about what the eigenvalues and eigenvectors mean, let's verify that our implementation is correct by checking the values against the built-in Julia [`eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a7ac1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.585973623539517e-5, 1.8581128760630783e-8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # compute the eigendecomposition using the built-in function -\n",
    "    F = eigen(Σ̂);\n",
    "    λ = F.values; # grab the eigenvalues\n",
    "    V = F.vectors; # grab the eigenvectors\n",
    "\n",
    "    # sort the eigenpairs by eigenvalue magnitude -\n",
    "    p = sortperm(λ)\n",
    "    λ = λ[p]\n",
    "    V = V[:,p]\n",
    "\n",
    "    # let's compare the eigenvalues -\n",
    "    ϵ = 1e-4; # tolerance for comparison\n",
    "    maximum_eigenvalue_delta = maximum(abs.(λ̂ - λ))\n",
    "    @assert maximum_eigenvalue_delta < ϵ \"Eigenvalues do not match within tolerance!\"\n",
    "\n",
    "    # let's compare the eigenvectors (up to a sign) -\n",
    "    ΔV = similar(V̂)\n",
    "    for i ∈ 1:length(λ̂)\n",
    "        v1 = V̂[:,i] / norm(V̂[:,i])\n",
    "        v2 = V[:,i] / norm(V[:,i])\n",
    "        if dot(v1, v2) < 0\n",
    "            v2 = -v2\n",
    "        end\n",
    "        ΔV[:,i] = v1 - v2\n",
    "    end\n",
    "    ϵv = 1e-3; # tolerance for eigenvector comparison\n",
    "    maximum_eigenvector_delta = maximum(abs.(ΔV))\n",
    "    @assert maximum_eigenvector_delta < ϵv \"Eigenvectors do not match within tolerance!\"\n",
    "\n",
    "    (maximum_eigenvalue_delta, maximum_eigenvector_delta)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1087e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One concise, direct summary sentence goes here\n",
    "\n",
    "> __Key Takeaways__\n",
    "> \n",
    "> Three key takeaways go here\n",
    "\n",
    "One concise, direct conclusion sentence goes here\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
