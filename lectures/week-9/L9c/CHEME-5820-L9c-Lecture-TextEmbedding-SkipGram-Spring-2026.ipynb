{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L9c: Skip-Gram Model for Word Embeddings\n",
    "In this lecture, we explore the Skip-Gram model, a method for learning word embeddings by predicting context words from a target word. Unlike CBOW (Continuous Bag-of-Words from L9a), which predicts a target from context, Skip-Gram reverses the task and predicts multiple context words given a single target word.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> \n",
    "> * __Skip-Gram architecture and training:__ Describe the Skip-Gram neural network structure, how it differs from CBOW, and the training objective that maximizes context word predictions.\n",
    "> * __Embedding extraction and usage:__ Identify where word embeddings are stored in Skip-Gram and explain why Skip-Gram produces better representations for rare words compared to CBOW.\n",
    "> * __Computational optimizations:__ Explain the softmax bottleneck in Skip-Gram and describe how Negative Sampling addresses this computational challenge.\n",
    "\n",
    "Let's get started!\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec77d00",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Today, we will use the following examples to illustrate key concepts:\n",
    "\n",
    "> [▶ Let's build a Skip Gram model](CHEME-5820-L9c-Example-SkipGram-Spring-2026.ipynb). In this example, we build a skip-gram model from scratch, train it on a small movie review corpus, and visualize the learned word embeddings.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740c17a",
   "metadata": {},
   "source": [
    "## Skip-Gram Model\n",
    "Skip-Gram reverses CBOW's prediction task: instead of predicting a target from context, it predicts multiple context words from a single target.\n",
    "\n",
    "> __What is it?__\n",
    ">\n",
    "> Skip-Gram uses a feedforward network with one hidden layer to predict context word probabilities given a target word. The input is the [one-hot encoded vector](https://en.wikipedia.org/wiki/One-hot) of the target word, and the output produces probability distributions for each context position within a sliding window.\n",
    "> \n",
    "> __Reference__: [Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space. ArXiv, abs/1301.3781.](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "Let $w_t$ be the target word at position $t$, $\\mathcal{C} = \\{t-m, \\dots, t-1, t+1, \\dots, t+m\\}$ the context positions within window size $m$, and $\\mathbf{v}_{w_t} \\in \\{0,1\\}^{N_{\\mathcal{V}}}$ the one-hot encoded target vector.\n",
    "\n",
    "The input $\\mathbf{x} = \\mathbf{v}_{w_t} \\in\\mathbb{R}^{N_{\\mathcal{V}}}$ connects to hidden layer $\\mathbf{h}\\in\\mathbb{R}^{h}$ via a linear transformation:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h} &= \\mathbf{W}_{1} \\cdot \\mathbf{x}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{W}_{1}\\in\\mathbb{R}^{h\\times{N_{\\mathcal{V}}}}$ is the input-to-hidden weight matrix. Since $\\mathbf{x}$ is one-hot, this selects the corresponding column of $\\mathbf{W}_{1}$. The hidden layer maps to the output layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u} &= \\mathbf{W}_{2} \\cdot \\mathbf{h}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{W}_{2}\\in\\mathbb{R}^{N_{\\mathcal{V}}\\times{h}}$ is the hidden-to-output weight matrix, producing $\\mathbf{u}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$. For each context position $c \\in \\mathcal{C}$, the model outputs a probability distribution over the vocabulary using softmax:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(w_{c} = w_{i} | w_t) = \\hat{y}_{c,i} &= \\frac{e^{u_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $u_i$ is the $i$-th component of the output vector $\\mathbf{u}$, and $\\hat{y}_{c,i}$ is the predicted probability that word $i$ appears at context position $c$ given target word $w_t$.\n",
    "\n",
    "> __Key Difference from CBOW__\n",
    ">\n",
    "> * **CBOW**: Many context words → one target (many-to-one)\n",
    "> * **Skip-Gram**: One target → many context words (one-to-many)\n",
    ">\n",
    "> Skip-Gram generates $2m$ predictions per example (one per context position), making it more expensive than CBOW but better for rare words since each occurrence produces $2m$ training signals.\n",
    "\n",
    "The training objective maximizes the likelihood of actual context words given the target. For target word $w_t$ with context $\\{w_{t-m}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+m}\\}$, we minimize negative log-likelihood:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= -\\log p(w_{t-m}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+m} | w_t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Assuming context words are conditionally independent given the target:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= -\\log \\prod_{c \\in \\mathcal{C}} p(w_c | w_t) \\\\\n",
    "&= -\\sum_{c \\in \\mathcal{C}} \\log p(w_c | w_t) \\\\\n",
    "&= -\\sum_{c \\in \\mathcal{C}} \\log \\left( \\frac{e^{u_{c}}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $u_{c}$ denotes the component of the output vector $\\mathbf{u}$ corresponding to the actual context word at position $c$. Expanding this:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= -\\sum_{c \\in \\mathcal{C}} \\left( u_{c} - \\log \\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j} \\right) \\\\\n",
    "&= \\sum_{c \\in \\mathcal{C}} \\left( \\log \\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j} - u_{c} \\right) \\\\\n",
    "&= |\\mathcal{C}| \\cdot \\log \\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j} - \\sum_{c \\in \\mathcal{C}} u_{c} \\quad\\blacksquare\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $|\\mathcal{C}| = 2m$ is the number of context positions. The softmax normalization term $\\log \\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j}$ appears $2m$ times, making this expensive to compute for large vocabularies.\n",
    "\n",
    "### Optimization Strategy\n",
    "We optimize the Skip-Gram objective using gradient descent to update the weight matrices $\\mathbf{W}_1$ and $\\mathbf{W}_2$.\n",
    "\n",
    "> __Training Procedure__\n",
    ">\n",
    "> Use stochastic gradient descent (SGD) with learning rate $\\alpha = 0.025$ that decays linearly to near zero over training. Process training examples in mini-batches, computing gradients for each example and accumulating them before updating parameters. Gradient descent scales to millions of parameters and billions of examples, achieving $O(h \\cdot k)$ complexity per update with negative sampling versus $O(h \\cdot N_{\\mathcal{V}})$ for full softmax. Adaptive optimizers (Adam, AdaGrad) can improve convergence for sparse gradients—see the advanced notebook for details.\n",
    "\n",
    "The gradient computation follows a three-step process:\n",
    "\n",
    "> __Gradient Computation__\n",
    ">\n",
    "> For a single context word $c$ in the context $\\mathcal{C}$ of target word $w_t$, we compute gradients of the loss $\\mathcal{L}_c = \\log \\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j} - u_c$ where $\\mathbf{u} = \\mathbf{W}_2 \\mathbf{h}$ and $\\mathbf{h} = \\mathbf{W}_1 \\mathbf{v}_{w_t}$.\n",
    ">\n",
    "> **Step 1: Gradient with respect to output vector $\\mathbf{u} \\in \\mathbb{R}^{N_{\\mathcal{V}}}$**\n",
    ">\n",
    "> The gradient is a vector where the $i$-th component is:\n",
    "> $$\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial u_i} = \\begin{cases}\n",
    "> \\hat{y}_i - 1 & \\text{if } i = c \\text{ (actual context word)} \\\\\n",
    "> \\hat{y}_i & \\text{if } i \\neq c \\text{ (all other words)}\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> where $\\hat{y}_i = \\frac{e^{u_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j}}$ is the softmax probability for word $i$. Collect these into the gradient vector:\n",
    "> $$\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{u}} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}_c}{\\partial u_1} \\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}_c}{\\partial u_{N_{\\mathcal{V}}}} \\end{bmatrix} = \\hat{\\mathbf{y}} - \\mathbf{e}_c \\in \\mathbb{R}^{N_{\\mathcal{V}}}\n",
    "> $$\n",
    "> where $\\hat{\\mathbf{y}} = [\\hat{y}_1, \\ldots, \\hat{y}_{N_{\\mathcal{V}}}]^{\\top}$ is the softmax output and $\\mathbf{e}_c$ is the one-hot vector for word $c$.\n",
    ">\n",
    "> **Step 2: Backpropagate to hidden layer**\n",
    ">\n",
    "> Using the chain rule with $\\mathbf{u} = \\mathbf{W}_2 \\mathbf{h}$:\n",
    "> $$\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{h}} = \\mathbf{W}_2^{\\top} \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{u}} \\in \\mathbb{R}^h\n",
    "> $$\n",
    ">\n",
    "> **Step 3: Compute weight gradients**\n",
    ">\n",
    "> Using the chain rule:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_2} &= \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{u}} \\mathbf{h}^{\\top} \\in \\mathbb{R}^{N_{\\mathcal{V}} \\times h} \\\\\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_1} &= \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{h}} \\mathbf{v}_{w_t}^{\\top} \\in \\mathbb{R}^{h \\times N_{\\mathcal{V}}}\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> Since $\\mathbf{v}_{w_t}$ is one-hot, $\\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_1}$ updates only the column corresponding to target word $w_t$.\n",
    "\n",
    "These gradients are then accumulated and used to update parameters:\n",
    "\n",
    "> __Parameter Update__\n",
    ">\n",
    "> For target word $w_t$ with context $\\mathcal{C} = \\{w_{t-m}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+m}\\}$, sum gradients across all $2m$ context positions:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\nabla_{\\mathbf{W}_1} \\mathcal{L} &= \\sum_{c \\in \\mathcal{C}} \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_1} \\\\\n",
    "> \\nabla_{\\mathbf{W}_2} \\mathcal{L} &= \\sum_{c \\in \\mathcal{C}} \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_2}\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> Then update parameters using gradient descent:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\mathbf{W}_1 &\\leftarrow \\mathbf{W}_1 - \\alpha \\nabla_{\\mathbf{W}_1} \\mathcal{L} \\\\\n",
    "> \\mathbf{W}_2 &\\leftarrow \\mathbf{W}_2 - \\alpha \\nabla_{\\mathbf{W}_2} \\mathcal{L}\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> For mini-batch training with $B$ examples, average gradients across all examples before updating.\n",
    "\n",
    "Skip-Gram costs $\\sim 2m \\times$ more than CBOW but provides better rare word embeddings because each rare word occurrence generates $2m$ gradient updates (one per context prediction) versus CBOW's single update. After training, extract word embeddings from the learned weight matrices:\n",
    "\n",
    "> __Embedding Extraction__\n",
    ">\n",
    "> Use embeddings from columns of $\\mathbf{W}_{1} \\in \\mathbb{R}^{h \\times N_{\\mathcal{V}}}$ (input embeddings) or rows of $\\mathbf{W}_{2} \\in \\mathbb{R}^{N_{\\mathcal{V}} \\times h}$ (output embeddings). In practice, use $\\mathbf{W}_{1}$ or the average of both matrices.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b870be1",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "The softmax denominator $\\sum_{j=1}^{N_{\\mathcal{V}}} e^{u_j}$ sums over all vocabulary words $2m$ times per example. For large vocabularies ($N_{\\mathcal{V}} \\approx 10^6$), this becomes prohibitive. Negative sampling replaces full softmax with binary classification: distinguish actual context words (positive examples) from randomly sampled words (negative examples).\n",
    "\n",
    "> __Negative Sampling Objective__\n",
    ">\n",
    "> For each context word $w_c$, let $\\mathbf{w}_{c}^{(2)}$ be the output embedding for $w_c$ (the corresponding row of $\\mathbf{W}_2$) and $\\mathbf{h}$ be the target word embedding. The objective becomes:\n",
    "> $$\n",
    "> \\log \\sigma((\\mathbf{w}_{c}^{(2)})^{\\top} \\mathbf{h}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-(\\mathbf{w}_{i}^{(2)})^{\\top} \\mathbf{h}) \\right]\n",
    "> $$\n",
    "> where $\\sigma(x) = 1/(1+e^{-x})$ is the sigmoid function, $k$ is the number of negative samples (typically 5-20), and $P_n(w)$ is the noise distribution (often $P_n(w) \\propto f(w)^{3/4}$ where $f(w)$ is word frequency).\n",
    "\n",
    "This reduces complexity from $O(N_{\\mathcal{V}})$ to $O(k)$ per context word. For target $w_t$ with context word $c$ and embedding $\\mathbf{h} = \\mathbf{W}_{1} \\cdot \\mathbf{v}_{w_t}$, compute:\n",
    "\n",
    "1. **Positive score**: $s_{+} = (\\mathbf{w}_{c}^{(2)})^{\\top} \\mathbf{h}$ for actual context word $w_c$\n",
    "2. **Negative scores**: $s_{-,i} = (\\mathbf{w}_{n_i}^{(2)})^{\\top} \\mathbf{h}$ for $k$ randomly sampled words $\\{w_{n_1}, \\dots, w_{n_k}\\}$\n",
    "3. **Combined loss**: $\\mathcal{L}_c = -\\log \\sigma(s_{+}) - \\sum_{i=1}^{k} \\log \\sigma(-s_{-,i})$\n",
    "\n",
    "The objective encourages high dot products for co-occurring words and low dot products for random pairs, learning that words appearing together should have similar embeddings.\n",
    "\n",
    "This approach has strong theoretical foundations:\n",
    "\n",
    "> __Theoretical Connection to PMI__\n",
    ">\n",
    "> [Levy & Goldberg (2014)](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html) showed that SGNS with optimal solutions is equivalent to factorizing:\n",
    "> $$\n",
    "> \\mathbf{M} = \\text{PMI}(w, c) - \\log k\n",
    "> $$\n",
    "> where PMI is Pointwise Mutual Information (discussed in L9a) and $k$ is the number of negative samples. This connects neural embedding methods to traditional count-based methods.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c16f5",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "Window size ($m=5$-$10$) controls context range: smaller captures syntax, larger captures semantics. Embedding dimension ($h=100$-$300$) balances expressiveness against data requirements. Negative samples ($k=5$-$20$) trades quality for computational speed.\n",
    "\n",
    "When choosing between CBOW and Skip-Gram:\n",
    "\n",
    "> __CBOW vs Skip-Gram Guidelines__\n",
    ">\n",
    "> * **Use CBOW** when training speed is critical and the corpus is large with frequent words\n",
    "> * **Use Skip-Gram** when rare word quality matters or the corpus is smaller\n",
    "> * **Skip-Gram** performs better on infrequent words because each occurrence generates $2m$ gradient updates instead of one\n",
    "\n",
    "Pre-trained embeddings: Word2Vec (Google News), GloVe (Common Crawl), FastText (with subwords). Use pre-trained for small (<10M words) or general corpora. Train from scratch for large, domain-specific corpora (medical, legal, technical).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879d31b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This lecture covers the Skip-Gram model for learning word embeddings by predicting context from target words.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * __Skip-Gram architecture:__ Skip-Gram reverses CBOW by predicting multiple context words from a single target word, producing $2m$ predictions per training example compared to CBOW's one prediction.\n",
    "> * __Rare word performance:__ Skip-Gram provides better embeddings for rare words because each occurrence generates $2m$ gradient updates to the word's embedding, while CBOW generates only one update per occurrence, allowing Skip-Gram to learn better representations from limited data.\n",
    "> * __Negative Sampling optimization:__ Negative Sampling replaces the expensive full softmax (complexity $O(N_{\\mathcal{V}})$) with a binary classification task using $k$ negative samples (complexity $O(k)$), making training practical for large vocabularies.\n",
    "\n",
    "Skip-Gram with Negative Sampling remains one of the most effective methods for learning word embeddings.\n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
