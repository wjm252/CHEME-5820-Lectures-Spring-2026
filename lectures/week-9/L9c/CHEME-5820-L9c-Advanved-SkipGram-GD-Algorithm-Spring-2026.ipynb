{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a76850",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithms for Skip-Gram Model\n",
    "The Skip-Gram model learns word embeddings by predicting context words from target words, optimizing parameters $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times N_{\\mathcal{V}}}$ and $\\mathbf{W}_2 \\in \\mathbb{R}^{N_{\\mathcal{V}} \\times h}$ using gradient-based optimization. This notebook examines gradient descent variants suitable for Skip-Gram training, particularly algorithms designed for high-dimensional sparse gradient problems.\n",
    "\n",
    "> __Gradient Descent for Skip-Gram__\n",
    ">\n",
    "> Standard gradient descent updates parameters using:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\theta_{k+1} &= \\theta_k - \\alpha \\nabla_{\\theta} \\mathcal{L}(\\theta_k)\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> where $\\theta$ represents all parameters $(\\mathbf{W}_1, \\mathbf{W}_2)$, $\\alpha > 0$ is the learning rate, and $\\nabla_{\\theta} \\mathcal{L}$ is the gradient of the loss function.\n",
    ">\n",
    "> For Skip-Gram, the loss for a single training example (target word $w_t$ with context $\\mathcal{C}$) is:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\mathcal{L} &= -\\sum_{c \\in \\mathcal{C}} \\log p(w_c | w_t)\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> where $p(w_c | w_t) = \\frac{\\exp(\\mathbf{u}_c)}{\\sum_{j=1}^{N_{\\mathcal{V}}} \\exp(\\mathbf{u}_j)}$ and $\\mathbf{u} = \\mathbf{W}_2 \\mathbf{h}$ with $\\mathbf{h} = \\mathbf{W}_1 \\mathbf{v}_{w_t}$.\n",
    "\n",
    "### Challenges in Skip-Gram Optimization\n",
    "Skip-Gram presents unique optimization challenges that motivate adaptive gradient methods:\n",
    "\n",
    "1. **Sparse gradients**: For each training example, only one column of $\\mathbf{W}_1$ receives gradient updates (the target word's embedding), and typically only $2m + k$ rows of $\\mathbf{W}_2$ receive updates (context words and negative samples). The remaining $N_{\\mathcal{V}} - 1$ columns of $\\mathbf{W}_1$ and $N_{\\mathcal{V}} - 2m - k$ rows of $\\mathbf{W}_2$ have zero gradients.\n",
    "\n",
    "2. **Varying update frequencies**: Frequent words receive many gradient updates, while rare words receive few. A single learning rate treats all parameters equally, causing frequent word embeddings to oscillate while rare word embeddings converge slowly.\n",
    "\n",
    "3. **High dimensionality**: With vocabulary sizes $N_{\\mathcal{V}} \\approx 10^4$ to $10^6$ and embedding dimension $h \\approx 100$ to $300$, the parameter space has $\\sim 10^7$ to $10^9$ dimensions.\n",
    "\n",
    "Adaptive gradient methods address these challenges by maintaining per-parameter learning rates that adjust based on gradient history.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5ca32",
   "metadata": {},
   "source": [
    "## AdaGrad: Adaptive Gradient Algorithm\n",
    "AdaGrad (Adaptive Gradient Algorithm) adapts learning rates for each parameter based on the history of squared gradients. Parameters with large cumulative gradients receive smaller learning rates, while parameters with small cumulative gradients receive larger learning rates.\n",
    "\n",
    "> __Algorithm__\n",
    ">\n",
    "> **Initialization**: Given initial parameters $\\theta_0$, learning rate $\\alpha > 0$, and small constant $\\epsilon > 0$ (typically $10^{-8}$) to prevent division by zero. Initialize the accumulated squared gradient $\\mathbf{G}_0 = \\mathbf{0}$ (same dimensions as $\\theta$). Set iteration counter $k \\gets 0$.\n",
    ">\n",
    "> For each training iteration $k = 0, 1, 2, \\ldots$ **do**:\n",
    "> 1. **Compute gradient**: $\\mathbf{g}_k = \\nabla_{\\theta} \\mathcal{L}(\\theta_k)$\n",
    "> 2. **Accumulate squared gradients**: $\\mathbf{G}_{k+1} = \\mathbf{G}_k + \\mathbf{g}_k \\odot \\mathbf{g}_k$ where $\\odot$ denotes element-wise multiplication\n",
    "> 3. **Update parameters**: $\\theta_{k+1} = \\theta_k - \\frac{\\alpha}{\\sqrt{\\mathbf{G}_{k+1}} + \\epsilon} \\odot \\mathbf{g}_k$ where division and square root are element-wise\n",
    "> 4. **Increment**: $k \\gets k + 1$\n",
    "\n",
    "The key innovation is the per-parameter learning rate $\\frac{\\alpha}{\\sqrt{G_{k+1,i}} + \\epsilon}$ for parameter $i$, where $G_{k+1,i}$ accumulates all past squared gradients for that parameter. This provides two benefits:\n",
    "\n",
    "1. **Automatic decay**: Parameters with large cumulative gradients get smaller effective learning rates\n",
    "2. **Sparse-friendly**: Parameters with zero gradients retain their full learning rate $\\alpha$\n",
    "\n",
    "> __Why AdaGrad for Skip-Gram?__\n",
    ">\n",
    "> AdaGrad naturally handles Skip-Gram's sparse gradient structure. Frequent words accumulate large $\\mathbf{G}$ values, reducing their learning rates and preventing oscillation. Rare words maintain small $\\mathbf{G}$ values, preserving large learning rates to extract maximum information from limited updates. This per-parameter adaptation requires no manual tuning of separate learning rates.\n",
    "\n",
    "### Limitations\n",
    "AdaGrad has a critical limitation: the accumulated squared gradient $\\mathbf{G}_k$ grows monotonically, causing learning rates to decay continuously. For parameter $i$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_{k,i} &= \\sum_{j=0}^{k-1} g_{j,i}^2\n",
    "\\end{align*}\n",
    "$$\n",
    "As $k \\to \\infty$, $G_{k,i} \\to \\infty$ (assuming non-zero gradients), so the effective learning rate $\\frac{\\alpha}{\\sqrt{G_{k,i}} + \\epsilon} \\to 0$. This aggressive decay can stop learning prematurely, particularly in non-convex problems where the optimization path may revisit similar parameter regions requiring continued adaptation.\n",
    "\n",
    "For Skip-Gram training over millions of examples, this monotonic decay often causes convergence to suboptimal solutions before reaching good word embeddings. This limitation motivated the development of methods that limit or discount old gradient information.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c470b",
   "metadata": {},
   "source": [
    "## Adam: Adaptive Moment Estimation\n",
    "Adam (Adaptive Moment Estimation) addresses AdaGrad's aggressive learning rate decay by using exponentially weighted moving averages of both gradients and squared gradients. This provides adaptive per-parameter learning rates while preventing indefinite decay.\n",
    "\n",
    "> __Algorithm__\n",
    ">\n",
    "> **Initialization**: Given initial parameters $\\theta_0$, learning rate $\\alpha > 0$ (typically $0.001$), exponential decay rates $\\beta_1 \\in [0,1)$ (typically $0.9$) for first moment and $\\beta_2 \\in [0,1)$ (typically $0.999$) for second moment, and small constant $\\epsilon > 0$ (typically $10^{-8}$). Initialize first moment $\\mathbf{m}_0 = \\mathbf{0}$, second moment $\\mathbf{v}_0 = \\mathbf{0}$, and iteration counter $k \\gets 1$.\n",
    ">\n",
    "> For each training iteration $k = 1, 2, 3, \\ldots$ **do**:\n",
    "> 1. **Compute gradient**: $\\mathbf{g}_k = \\nabla_{\\theta} \\mathcal{L}(\\theta_{k-1})$\n",
    "> 2. **Update biased first moment**: $\\mathbf{m}_k = \\beta_1 \\mathbf{m}_{k-1} + (1 - \\beta_1) \\mathbf{g}_k$\n",
    "> 3. **Update biased second moment**: $\\mathbf{v}_k = \\beta_2 \\mathbf{v}_{k-1} + (1 - \\beta_2) \\mathbf{g}_k \\odot \\mathbf{g}_k$\n",
    "> 4. **Bias correction for first moment**: $\\hat{\\mathbf{m}}_k = \\frac{\\mathbf{m}_k}{1 - \\beta_1^k}$\n",
    "> 5. **Bias correction for second moment**: $\\hat{\\mathbf{v}}_k = \\frac{\\mathbf{v}_k}{1 - \\beta_2^k}$\n",
    "> 6. **Update parameters**: $\\theta_k = \\theta_{k-1} - \\frac{\\alpha}{\\sqrt{\\hat{\\mathbf{v}}_k} + \\epsilon} \\odot \\hat{\\mathbf{m}}_k$\n",
    "> 7. **Increment**: $k \\gets k + 1$\n",
    "\n",
    "Adam maintains two moving averages: $\\mathbf{m}_k$ estimates the mean gradient (first moment), and $\\mathbf{v}_k$ estimates the uncentered variance (second moment). The bias correction steps account for initialization at zero, which biases estimates toward zero in early iterations.\n",
    "\n",
    "> __Comparison to AdaGrad__\n",
    ">\n",
    "> AdaGrad accumulates all past squared gradients: $\\mathbf{G}_k = \\sum_{j=1}^{k} \\mathbf{g}_j \\odot \\mathbf{g}_j$, causing monotonic growth. Adam uses exponential moving average: $\\mathbf{v}_k = \\beta_2 \\mathbf{v}_{k-1} + (1 - \\beta_2) \\mathbf{g}_k \\odot \\mathbf{g}_k$, giving recent gradients weight $(1-\\beta_2)$ and discounting old gradients by $\\beta_2$ per iteration. With $\\beta_2 = 0.999$, gradients from $\\sim 1000$ iterations ago contribute negligibly ($0.999^{1000} \\approx 0.37$), preventing indefinite accumulation while maintaining adaptation to recent gradient patterns.\n",
    "\n",
    "### Why Adam for Skip-Gram?\n",
    "Adam combines three properties beneficial for Skip-Gram training:\n",
    "\n",
    "1. **Momentum-like behavior**: The first moment $\\mathbf{m}_k$ smooths noisy gradients from stochastic mini-batches, accelerating convergence in consistent directions\n",
    "2. **Adaptive learning rates**: The second moment $\\mathbf{v}_k$ provides per-parameter scaling similar to AdaGrad, handling sparse gradients effectively\n",
    "3. **Bounded decay**: Exponential averaging prevents the vanishing learning rates that plague AdaGrad in long training runs\n",
    "\n",
    "The effective learning rate for parameter $i$ at iteration $k$ is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_{\\text{eff},i,k} &= \\frac{\\alpha}{\\sqrt{\\hat{v}_{k,i}} + \\epsilon}\n",
    "\\end{align*}\n",
    "$$\n",
    "Unlike AdaGrad where $\\alpha_{\\text{eff},i,k}$ decreases monotonically, Adam's $\\alpha_{\\text{eff},i,k}$ can increase or decrease based on recent gradient history, allowing the algorithm to adapt to changing loss landscapes during training.\n",
    "\n",
    "> __Practical Considerations__\n",
    ">\n",
    "> For Skip-Gram training, Adam typically uses default hyperparameters ($\\alpha = 0.001$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$) and requires minimal tuning. Start with these defaults and reduce $\\alpha$ if training diverges. The memory overhead is $2 \\times \\text{sizeof}(\\theta)$ to store $\\mathbf{m}$ and $\\mathbf{v}$, acceptable for embedding dimensions $h \\approx 100$-$300$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d3eed",
   "metadata": {},
   "source": [
    "## Application to Skip-Gram Training\n",
    "This section details how Adam and AdaGrad apply to Skip-Gram parameter updates with negative sampling.\n",
    "\n",
    "### Gradient Computation for Skip-Gram\n",
    "For a single training example with target word $w_t$ and context position $c$, the loss with negative sampling is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_c &= -\\log \\sigma((\\mathbf{w}_c^{(2)})^{\\top} \\mathbf{h}) - \\sum_{i=1}^{k} \\log \\sigma(-(\\mathbf{w}_{n_i}^{(2)})^{\\top} \\mathbf{h})\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{h} = \\mathbf{W}_1 \\mathbf{v}_{w_t}$ is the target word embedding (column of $\\mathbf{W}_1$), $\\mathbf{w}_c^{(2)}$ is the context word's output embedding (row of $\\mathbf{W}_2$), $\\mathbf{w}_{n_i}^{(2)}$ are negative sample embeddings, $k$ is the number of negative samples, and $\\sigma(x) = 1/(1+e^{-x})$ is the sigmoid function.\n",
    "\n",
    "The total loss for this target word sums over all $|\\mathcal{C}| = 2m$ context positions:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\sum_{c \\in \\mathcal{C}} \\mathcal{L}_c\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> __Gradients__\n",
    ">\n",
    "> For context position $c$ with positive example $w_c$ and negative samples $\\{w_{n_1}, \\ldots, w_{n_k}\\}$:\n",
    ">\n",
    "> **Gradient with respect to output embeddings**:\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{w}_c^{(2)}} &= (\\sigma((\\mathbf{w}_c^{(2)})^{\\top} \\mathbf{h}) - 1) \\mathbf{h} \\in \\mathbb{R}^h \\\\\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{w}_{n_i}^{(2)}} &= \\sigma((\\mathbf{w}_{n_i}^{(2)})^{\\top} \\mathbf{h}) \\mathbf{h} \\in \\mathbb{R}^h \\quad \\text{for } i = 1, \\ldots, k\n",
    "> \\end{align*}\n",
    "> $$\n",
    ">\n",
    "> **Gradient with respect to hidden layer** (target embedding):\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{h}} &= (\\sigma((\\mathbf{w}_c^{(2)})^{\\top} \\mathbf{h}) - 1) \\mathbf{w}_c^{(2)} + \\sum_{i=1}^{k} \\sigma((\\mathbf{w}_{n_i}^{(2)})^{\\top} \\mathbf{h}) \\mathbf{w}_{n_i}^{(2)} \\in \\mathbb{R}^h\n",
    "> \\end{align*}\n",
    "> $$\n",
    ">\n",
    "> **Gradient with respect to input embeddings** (propagated to $\\mathbf{W}_1$):\n",
    "> $$\n",
    "> \\begin{align*}\n",
    "> \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{W}_1} &= \\frac{\\partial \\mathcal{L}_c}{\\partial \\mathbf{h}} \\mathbf{v}_{w_t}^{\\top} \\in \\mathbb{R}^{h \\times N_{\\mathcal{V}}}\n",
    "> \\end{align*}\n",
    "> $$\n",
    "> Since $\\mathbf{v}_{w_t}$ is one-hot, this updates only the column of $\\mathbf{W}_1$ corresponding to target word $w_t$.\n",
    "\n",
    "### Parameter Update with Adam\n",
    "For mini-batch of $B$ training examples, accumulate gradients:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{g}_{\\mathbf{W}_1,k} &= \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{c \\in \\mathcal{C}_b} \\frac{\\partial \\mathcal{L}_{b,c}}{\\partial \\mathbf{W}_1} \\\\\n",
    "\\mathbf{g}_{\\mathbf{W}_2,k} &= \\frac{1}{B} \\sum_{b=1}^{B} \\sum_{c \\in \\mathcal{C}_b} \\left( \\frac{\\partial \\mathcal{L}_{b,c}}{\\partial \\mathbf{w}_{c}^{(2)}} + \\sum_{i=1}^{k} \\frac{\\partial \\mathcal{L}_{b,c}}{\\partial \\mathbf{w}_{n_i}^{(2)}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Apply Adam updates separately to $\\mathbf{W}_1$ and $\\mathbf{W}_2$:\n",
    "\n",
    "1. **Update moment estimates**:\n",
    "   - $\\mathbf{m}_{\\mathbf{W}_1,k} = \\beta_1 \\mathbf{m}_{\\mathbf{W}_1,k-1} + (1-\\beta_1) \\mathbf{g}_{\\mathbf{W}_1,k}$\n",
    "   - $\\mathbf{v}_{\\mathbf{W}_1,k} = \\beta_2 \\mathbf{v}_{\\mathbf{W}_1,k-1} + (1-\\beta_2) \\mathbf{g}_{\\mathbf{W}_1,k} \\odot \\mathbf{g}_{\\mathbf{W}_1,k}$\n",
    "   - (Similarly for $\\mathbf{W}_2$)\n",
    "\n",
    "2. **Bias correction**:\n",
    "   - $\\hat{\\mathbf{m}}_{\\mathbf{W}_1,k} = \\frac{\\mathbf{m}_{\\mathbf{W}_1,k}}{1-\\beta_1^k}$, $\\hat{\\mathbf{v}}_{\\mathbf{W}_1,k} = \\frac{\\mathbf{v}_{\\mathbf{W}_1,k}}{1-\\beta_2^k}$\n",
    "\n",
    "3. **Parameter update**:\n",
    "   - $\\mathbf{W}_1 \\leftarrow \\mathbf{W}_1 - \\frac{\\alpha}{\\sqrt{\\hat{\\mathbf{v}}_{\\mathbf{W}_1,k}} + \\epsilon} \\odot \\hat{\\mathbf{m}}_{\\mathbf{W}_1,k}$\n",
    "   - (Similarly for $\\mathbf{W}_2$)\n",
    "\n",
    "> __Sparsity Handling__\n",
    ">\n",
    "> In each mini-batch, only embeddings for observed words (targets and their contexts/negative samples) receive non-zero gradients. Adam's moment estimates for unobserved words remain unchanged, automatically handling sparsity without special logic. This contrasts with standard SGD where all parameters would receive equal learning rates regardless of update frequency.\n",
    "\n",
    "### Convergence Criteria\n",
    "Training typically continues for a fixed number of epochs (passes through the corpus) rather than until convergence, as word embeddings continue improving even with small gradient norms. Common stopping conditions:\n",
    "\n",
    "1. **Fixed epochs**: Train for $E$ epochs where $E \\in \\{5, 10, 20\\}$ depending on corpus size\n",
    "2. **Validation loss**: Monitor loss on held-out data and stop if loss increases for consecutive epochs (early stopping)\n",
    "3. **Embedding stability**: Measure cosine similarity between embeddings at epoch $t$ and $t-1$; stop if similarity exceeds threshold (e.g., $0.99$)\n",
    "\n",
    "For Skip-Gram with negative sampling and Adam, convergence to a tolerance $\\|\\mathbf{g}_k\\|_2 \\leq \\epsilon$ is rarely used, as gradient norms remain large due to the stochastic nature of negative sampling even near good solutions.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
