{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b73d96",
   "metadata": {},
   "source": [
    "# Example: Bag of Words, TF-IDF and PMI\n",
    "In this example, we'll play around with simple collections of text data and explore how to create basic text embeddings using the Bag of Words model, Term Frequency-Inverse Document Frequency (TF-IDF), and Pointwise Mutual Information (PMI).\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this example, you should be able to:\n",
    ">\n",
    "> * __Bag of Words construction:__ Construct Bag of Words representations for text data using both dictionary-based and hashing-based vectorization methods.\n",
    "> * __TF-IDF computation:__ Compute Term Frequency-Inverse Document Frequency (TF-IDF) scores to re-weight word counts and identify distinctive terms in a document.\n",
    "> * __PMI estimation:__ Estimate Pointwise Mutual Information (PMI) and Positive PMI (PPMI) matrices from corpus statistics to quantify word associations.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9a118",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants.\n",
    "\n",
    "> The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a8b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5046110",
   "metadata": {},
   "source": [
    "### Data\n",
    "Here, let's construct a small corpus of simple sentences to work with in the `sentences::Array{String,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98193ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{String}:\n",
       " \"I love machine learning and data science .\"\n",
       " \"Machine learning is fun .\"\n",
       " \"Machine learning is great .\"\n",
       " \"I love coding machine learning in Julia !\"\n",
       " \"Julia is a great programming language ?\"\n",
       " \"I enjoy learning new things abo\"\u001b[93m\u001b[1m ⋯ 35 bytes ⋯ \u001b[22m\u001b[39m\", and artificial intelligence .\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = let\n",
    "\n",
    "    # initialize -\n",
    "    sentences_array = Array{String,1}(); # initialize an array of sentence strings\n",
    "\n",
    "    # add sentences -\n",
    "    push!(sentences_array, \"I love machine learning and data science .\"); #1 the first sentence is different than the others\n",
    "    push!(sentences_array, \"Machine learning is fun .\"); # the second sentence is close to the third sentence\n",
    "    push!(sentences_array, \"Machine learning is great .\"); # expect similarity b/w 2nd and 3rd sentences\n",
    "    push!(sentences_array, \"I love coding machine learning in Julia !\"); # 4 the fourth sentence is similar to the first sentence\n",
    "    push!(sentences_array, \"Julia is a great programming language ?\");\n",
    "    push!(sentences_array, \"I enjoy learning new things about data science , machine learning , and artificial intelligence .\");\n",
    "    \n",
    "    # return the array\n",
    "    sentences_array\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a3828",
   "metadata": {},
   "source": [
    "Next, we'll preprocess the data in the `sentences::Array{String,1}` variable by tokenizing the sentences into words, and converting them to lowercase. This, along with our control tokens, will be our vocabulary model.\n",
    "\n",
    "We'll store the vocabulary in the `vocabulary::Dict{String, Int64}` variable, where the keys are the unique words and the values are their corresponding indices, and the inverse vocabulary in the `inverse_vocabulary::Dict{Int64, String}` variable, where the keys are the indices and the values are the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8047e1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(\"!\" => 1, \"is\" => 17, \"enjoy\" => 11, \"data\" => 10, \"language\" => 19, \"coding\" => 9, \"science\" => 25, \"<bos>\" => 27, \"a\" => 5, \"and\" => 7…), Dict(5 => \"a\", 16 => \"intelligence\", 20 => \"learning\", 12 => \"fun\", 24 => \"programming\", 28 => \"<eos>\", 8 => \"artificial\", 17 => \"is\", 30 => \"<unk>\", 1 => \"!\"…))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, inverse_vocabulary = let\n",
    "\n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int64}(); # initialize the vocabulary dictionary\n",
    "    inverse_vocabulary = Dict{Int64, String}(); # initialize the inverse vocabulary dictionary\n",
    "    index = 1; # initialize the index counter\n",
    "\n",
    "    # control tokens -\n",
    "    control_tokens = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]; # define control tokens\n",
    "\n",
    "    # tmp variables -\n",
    "    words = Set{String}(); # temporary array to hold words\n",
    "    for sentence in sentences\n",
    "        tmp = split(lowercase(sentence)); # convert to lowercase and split by whitespace\n",
    "        push!(words, tmp...); # add words to the set\n",
    "    end\n",
    "    words_array = collect(words) |> sort; # convert set to array, and sort it\n",
    "\n",
    "    # append the control tokens to the words array\n",
    "    words_array = vcat(words_array, control_tokens);\n",
    "    for word in words_array\n",
    "        vocabulary[word] = index;\n",
    "        inverse_vocabulary[index] = word;\n",
    "        index += 1;\n",
    "    end\n",
    "\n",
    "    # return the vocabulary and inverse vocabulary\n",
    "    (vocabulary, inverse_vocabulary)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54542876",
   "metadata": {},
   "source": [
    "What's in the `vocabulary::Dict{String, Int64}` and `inverse_vocabulary::Dict{Int64, String}` variables? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5166d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Int64} with 30 entries:\n",
       "  \"!\"            => 1\n",
       "  \"is\"           => 17\n",
       "  \"enjoy\"        => 11\n",
       "  \"data\"         => 10\n",
       "  \"language\"     => 19\n",
       "  \"coding\"       => 9\n",
       "  \"science\"      => 25\n",
       "  \"<bos>\"        => 27\n",
       "  \"a\"            => 5\n",
       "  \"and\"          => 7\n",
       "  \",\"            => 2\n",
       "  \"programming\"  => 24\n",
       "  \"love\"         => 21\n",
       "  \"?\"            => 4\n",
       "  \".\"            => 3\n",
       "  \"in\"           => 15\n",
       "  \"i\"            => 14\n",
       "  \"<unk>\"        => 30\n",
       "  \"about\"        => 6\n",
       "  \"<pad>\"        => 29\n",
       "  \"machine\"      => 22\n",
       "  \"artificial\"   => 8\n",
       "  \"learning\"     => 20\n",
       "  \"intelligence\" => 16\n",
       "  \"great\"        => 13\n",
       "  ⋮              => ⋮"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e3fc4",
   "metadata": {},
   "source": [
    "What can we do with these vocabulary models? One thing we can do is take a sentence, and convert it into an index vector using the vocabulary model. Let's check that out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = \"I love machine learning and data science .\"\n",
      "words = [\"<bos>\", \"i\", \"love\", \"machine\", \"learning\", \"and\", \"data\", \"science\", \".\", \"<eos>\"]\n",
      "word_indices = [27, 14, 21, 22, 20, 7, 10, 25, 3, 28]\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    i = 1; # select a sentence index to inspect\n",
    "    sentence = sentences[i]; # get the sentence\n",
    "\n",
    "    augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "    words = split(lowercase(augmented_sentence)) .|> String;\n",
    "    word_indices = [get(vocabulary, word, vocabulary[\"<unk>\"]) for word in words]; # wow! nice one-liner to get indices with <unk> fallback\n",
    "\n",
    "    @show sentence;\n",
    "    @show words;\n",
    "    @show word_indices;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb9a93",
   "metadata": {},
   "source": [
    "### Helper Implementations\n",
    "We implement a helper function `hashing_vectorizer` to convert a list of features (words) into a fixed-length vector using the hashing trick. This function maps each feature to an index using a hash function and increments the count at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8beedf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashing_vectorizer (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hashing_vectorizer(features::Array{String,1}; length::Int64 = 10)::Array{Int64,1}\n",
    "\n",
    "    # initialize -\n",
    "    new_hash_vector = zeros(Int,length);\n",
    "    for i ∈ eachindex(features)\n",
    "        feature = features[i]; # get feature\n",
    "        j = hash(feature) |> h-> mod1(h, length); # this gives us an index in 1:length (Julia's 1-based indexing)\n",
    "        new_hash_vector[j] += 1;\n",
    "    end\n",
    "   \n",
    "    new_hash_vector; # return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a69a4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6e323",
   "metadata": {},
   "source": [
    "## Task 1: Bag of Words Representations\n",
    "In this task, we'll create a Bag of Words representation for our corpus of sentences. \n",
    "\n",
    "> __Bag of Words Model__\n",
    ">\n",
    "> The Bag of Words (BoW) model is a technique for text embedding. As the name suggests, we represent a text (such as a sentence or a document) as a \"bag\" (multiset) of its words, disregarding grammar and word order but keeping multiplicity. Given a vocabulary of unique words, each text is represented as a vector where each dimension corresponds to a word in the vocabulary, and the value in that dimension indicates the frequency of that word in the text.\n",
    "\n",
    "Let's compute the Bag of Words representation for each sentence in our corpus and store the results in the `bow_matrix::Array{Int64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2bae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×30 Matrix{Int64}:\n",
       " 0  0  1  0  0  0  1  0  0  1  0  0  0  …  0  1  1  1  0  0  1  0  1  1  0  0\n",
       " 0  0  1  0  0  0  0  0  0  0  0  1  0     0  1  0  1  0  0  0  0  1  1  0  0\n",
       " 0  0  1  0  0  0  0  0  0  0  0  0  1     0  1  0  1  0  0  0  0  1  1  0  0\n",
       " 1  0  0  0  0  0  0  0  1  0  0  0  0     0  1  1  1  0  0  0  0  1  1  0  0\n",
       " 0  0  0  1  1  0  0  0  0  0  0  0  1     1  0  0  0  0  1  0  0  1  1  0  0\n",
       " 0  2  1  0  0  1  1  1  0  1  1  0  0  …  0  2  0  1  1  0  1  1  1  1  0  0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = length(sentences); # number of sentences\n",
    "    vocab_size = length(vocabulary); # size of the vocabulary\n",
    "    bow_matrix = zeros(Int64, num_sentences, vocab_size); # initialize the Bag of Words matrix\n",
    "\n",
    "    # populate the Bag of Words matrix -\n",
    "    for (i, sentence) in enumerate(sentences)\n",
    "\n",
    "        # add the <BOS> ... <EOS> token wrappers\n",
    "        augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "        words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "        for word in words\n",
    "            if haskey(vocabulary, word)\n",
    "                index = vocabulary[word];\n",
    "                bow_matrix[i, index] += 1; # increment the count for the word\n",
    "            else\n",
    "                unk_index = vocabulary[\"<unk>\"];\n",
    "                bow_matrix[i, unk_index] += 1; # increment the count for unknown words\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the Bag of Words matrix\n",
    "    bow_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1486fe",
   "metadata": {},
   "source": [
    "### What is wrong with this representation? \n",
    "There are some issues with the Bag of Words representation.\n",
    "\n",
    "> __So what's the problem?__\n",
    ">\n",
    "> We had text, now we have numerical vectors. Great! Why are you complaining?\n",
    "> \n",
    "> * __Sparsity and dimension:__ The counts require us to maintain a vocabulary dictionary that maps words to indices in our vectors. But suppose we have a very large vocabulary. Further, most of the entries in our vectors are zero. This is called a \"sparse\" representation, and it can be inefficient in terms of both storage and computation. \n",
    "> \n",
    "> * __Order:__ We also have to impose some order on our vocabulary (e.g., alphabetical order) to ensure that the indices are consistent across different texts. This can be cumbersome, especially when dealing with large and dynamic vocabularies.\n",
    "> \n",
    "> Let's consider an alternative representation. \n",
    "\n",
    "Instead of maintaining a vocabulary dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying [a hash function `h(...)`](https://docs.julialang.org/en/v1/base/base/#Base.hash) to the features (e.g., words). The hash function uses these values directly as feature indices, incrementing counts in the resulting vector at those indices. \n",
    "\n",
    "> __Hash function?__\n",
    "> \n",
    "> A hash function is a function that takes an input (or 'message') and returns a fixed-size string of bytes. The output, typically a hash code or hash value, is deterministic (same input always produces the same output) but not necessarily unique (different inputs can produce the same hash, called a collision). Hash functions are commonly used in computer science for tasks such as data retrieval, cryptography, and data integrity verification.\n",
    "\n",
    "We've implemented a simple version of a hashing vectorizer in the `hashing_vectorizer(...)` function that uses [Julia's built-in `hash(...)` function](https://docs.julialang.org/en/v1/base/base/#Base.hash). We pass in a list of words (from a sentence) and the desired length of the output vector, and the function returns a vector of the specified length with counts of the hashed words.\n",
    "\n",
    "We save this output in the `vectorized_sentence::Array{Int64,1}` variable. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681ccd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30-element Vector{Int64}:\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_sentence = let\n",
    "\n",
    "    # initialize -\n",
    "    i = 1; # index of sentence to hash\n",
    "    sentence = sentences[i];\n",
    "    augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "    words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "    tv = hashing_vectorizer(words, length = length(vocabulary));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80325667",
   "metadata": {},
   "source": [
    "You can change the sentence index `i` in the code block above to see how different sentences are represented using this hashing vectorizer. Further, you can modify the `length` parameter to see how the size of the output vector affects the representation.\n",
    "\n",
    "Play around with different sentences and vector lengths to see how the hashing vectorizer performs! We'll save this output in the `alternative_vectorized_sentence::Array{Int64,1}` variable. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f382f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Int64}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_vectorized_sentence = let\n",
    "\n",
    "    # initialize -\n",
    "    i = 2; # index of sentence to hash\n",
    "    sentence = sentences[i];\n",
    "    desired_sentence_length = 10; # desired length of the output vector\n",
    "    augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "    words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "    tv = hashing_vectorizer(words, length = desired_sentence_length);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e7121",
   "metadata": {},
   "source": [
    "### Hash Collisions\n",
    "When using the hashing trick, multiple words may map to the same index in the output vector. This is called a **collision**.\n",
    "\n",
    "> __What happens with collisions?__\n",
    ">\n",
    "> *   **Information loss**: When two different words map to the same index, their counts are combined, and the model cannot distinguish between them.\n",
    "> *   **Dimensionality trade-off**: Smaller vector lengths increase the probability of collisions but reduce memory usage. Larger vector lengths reduce collisions but increase sparsity and memory usage.\n",
    "> *   **Mitigation**: In practice, we choose a vector length large enough (e.g., $2^{18}$ or $2^{20}$) to minimize collisions for the given vocabulary size.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df41bf4",
   "metadata": {},
   "source": [
    "## Task 2: TF-IDF Representations\n",
    "In this task, we'll compute the Term Frequency-Inverse Document Frequency (TF-IDF) representation for our corpus of sentences. The TF-IDF score for a term $t$ in a document $d$ is given by the product of two terms:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF}(t, d) &= \\text{tf}(t, d) \\cdot \\text{idf}(t, \\mathcal{D})\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "The __Term Frequency__ ($\\text{tf}$) is the raw count of term $t$ in document $d$, often normalized by the total number of words in $d$. The __Inverse Document Frequency__ ($\\text{idf}$) measures how much the term is tied to a subset of documents. It is calculated as:\n",
    "    $$ \\text{idf}(t, \\mathcal{D}) = \\ln \\left( \\frac{N}{|\\{d \\in \\mathcal{D} : t \\in d\\}|} \\right) $$\n",
    "where $N$ is the total number of documents in the corpus $\\mathcal{D}$, and the denominator is the number of documents where the term $t$ appears. In practice (especially for small corpora), we often use a smoothed IDF to avoid division-by-zero and to reduce extreme values; we'll use that smoothed form below.\n",
    "\n",
    "First, let's compute the TF values for each term in our vocabulary for each sentence in the corpus. We'll store these values in the `TF_matrix::Array{Float64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85262330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×30 Matrix{Float64}:\n",
       " 0.0  0.0       0.1        0.0       …  0.1        0.1        0.0  0.0\n",
       " 0.0  0.0       0.142857   0.0          0.142857   0.142857   0.0  0.0\n",
       " 0.0  0.0       0.142857   0.0          0.142857   0.142857   0.0  0.0\n",
       " 0.1  0.0       0.0        0.0          0.1        0.1        0.0  0.0\n",
       " 0.0  0.0       0.0        0.111111     0.111111   0.111111   0.0  0.0\n",
       " 0.0  0.111111  0.0555556  0.0       …  0.0555556  0.0555556  0.0  0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = size(bow_matrix, 1); # number of sentences\n",
    "    vocab_size = size(bow_matrix, 2); # size of the vocabulary\n",
    "    TF_matrix = zeros(Float64, num_sentences, vocab_size); # initialize the TF matrix\n",
    "\n",
    "    # populate the TF matrix -\n",
    "    for i in 1:num_sentences\n",
    "        total_terms = sum(bow_matrix[i, :]);\n",
    "        if total_terms == 0\n",
    "            continue\n",
    "        end\n",
    "        for j in 1:vocab_size\n",
    "            TF_matrix[i, j] = bow_matrix[i, j] / total_terms;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the TF matrix\n",
    "    TF_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7bff3",
   "metadata": {},
   "source": [
    "Next, let's compute the IDF values for each term in our vocabulary across the entire corpus. We'll store these values in the `IDF_values_dictionary::Dict{String, Float64}` variable, where the keys are the unique words and the values are their corresponding IDF scores.\n",
    "\n",
    "To keep IDF values finite in small corpora (and to avoid division-by-zero when a term appears in zero documents), we'll use a smoothed IDF:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{idf}(t, \\mathcal{D}) &= \\ln \\left( \\frac{N + 1}{|\\{d \\in \\mathcal{D} : t \\in d\\}| + 1} \\right)\n",
    "\\end{align*}}\n",
    "$$\n",
    "where $N$ is the total number of documents in the corpus $\\mathcal{D}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da8d4588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Float64} with 30 entries:\n",
       "  \"!\"            => 1.25276\n",
       "  \"is\"           => 0.559616\n",
       "  \"enjoy\"        => 1.25276\n",
       "  \"data\"         => 0.847298\n",
       "  \"language\"     => 1.25276\n",
       "  \"coding\"       => 1.25276\n",
       "  \"science\"      => 0.847298\n",
       "  \"<bos>\"        => 0.0\n",
       "  \"a\"            => 1.25276\n",
       "  \"and\"          => 0.847298\n",
       "  \",\"            => 1.25276\n",
       "  \"programming\"  => 1.25276\n",
       "  \"love\"         => 0.847298\n",
       "  \"?\"            => 1.25276\n",
       "  \".\"            => 0.336472\n",
       "  \"in\"           => 1.25276\n",
       "  \"i\"            => 0.559616\n",
       "  \"<unk>\"        => 1.94591\n",
       "  \"about\"        => 1.25276\n",
       "  \"<pad>\"        => 1.94591\n",
       "  \"machine\"      => 0.154151\n",
       "  \"artificial\"   => 1.25276\n",
       "  \"learning\"     => 0.154151\n",
       "  \"intelligence\" => 1.25276\n",
       "  \"great\"        => 0.847298\n",
       "  ⋮              => ⋮"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_values_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = length(sentences); # number of sentences\n",
    "    IDF_values_dictionary = Dict{String, Float64}(); # initialize dictionary of IDF values\n",
    "\n",
    "    # compute smoothed IDF for each term in the vocabulary -\n",
    "    for (word, index) in vocabulary\n",
    "        doc_frequency = sum(bow_matrix[:, index] .> 0);\n",
    "        IDF_values_dictionary[word] = log((num_sentences + 1) / (doc_frequency + 1));\n",
    "    end\n",
    "\n",
    "    # return dictionary\n",
    "    IDF_values_dictionary\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1c5c2",
   "metadata": {},
   "source": [
    "Finally, we can compute the TF-IDF representation for each sentence in our corpus by multiplying the TF values with the corresponding IDF values. We'll store the resulting TF-IDF representations in the `TFIDF_matrix::Array{Float64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd8baf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×30 Matrix{Float64}:\n",
       " 0.0       0.0       0.0336472  0.0       …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0       0.0       0.0480675  0.0          0.0        0.0  0.0  0.0  0.0\n",
       " 0.0       0.0       0.0480675  0.0          0.0        0.0  0.0  0.0  0.0\n",
       " 0.125276  0.0       0.0        0.0          0.0        0.0  0.0  0.0  0.0\n",
       " 0.0       0.0       0.0        0.139196     0.0        0.0  0.0  0.0  0.0\n",
       " 0.0       0.139196  0.0186929  0.0       …  0.0695979  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = size(bow_matrix, 1); # number of sentences\n",
    "    vocab_size = size(bow_matrix, 2); # size of the vocabulary\n",
    "    TFIDF_matrix = zeros(Float64, num_sentences, vocab_size); # initialize the TF-IDF matrix\n",
    "\n",
    "    # populate the TF-IDF matrix -\n",
    "    for i in 1:num_sentences\n",
    "        for j in 1:vocab_size\n",
    "            word = inverse_vocabulary[j];\n",
    "            idf_value = IDF_values_dictionary[word];\n",
    "            TFIDF_matrix[i, j] = TF_matrix[i, j] * idf_value;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the TF-IDF matrix\n",
    "    TFIDF_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225f2b9",
   "metadata": {},
   "source": [
    "So what does the TF-IDF representation tell us about our sentences? Let's compute the similarity between the TF-IDF vectors of two sentences to see how similar they are in terms of their content. \n",
    "\n",
    "> __Test:__ Let's compute the cosine similarity between the TF and the TF-IDF vectors of sentence 2 and sentence 3 (we know these sentences share some words) versus sentence 1 and sentence 5 (we know these sentences do not share any words). Vector 2 and 3 should be more similar than Vector 1 and 5.\n",
    "\n",
    "What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85fe016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between sentence 2 and sentence 3: 0.3036826972774764\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    i = 2; # index of first sentence to inspect\n",
    "    j = 3; # index of second sentence to inspect\n",
    "    D = TFIDF_matrix; # you can use TF-IDF or TF matrix for this example\n",
    "\n",
    "    # get the TF-IDF vectors for the two sentences\n",
    "    vᵢ = D[i, :];\n",
    "    vⱼ = D[j, :];\n",
    "\n",
    "    dot_product = dot(vᵢ, vⱼ);\n",
    "    magnitude_vᵢ = norm(vᵢ);\n",
    "    magnitude_vⱼ = norm(vⱼ);\n",
    "    \n",
    "    if magnitude_vᵢ == 0 || magnitude_vⱼ == 0\n",
    "        return 0.0 # Return 0 for vectors with zero magnitude\n",
    "    end\n",
    "    \n",
    "    value = dot_product / (magnitude_vᵢ * magnitude_vⱼ);\n",
    "    println(\"Cosine similarity between sentence $i and sentence $j: $value\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d588c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced0d04",
   "metadata": {},
   "source": [
    "## Task 3: PMI Representations\n",
    "In this task, we'll compute a Pointwise Mutual Information (PMI) representation for our corpus using word co-occurrence counts within a fixed context window.\n",
    "\n",
    "> __Pointwise Mutual Information (PMI)__\n",
    ">\n",
    "> PMI compares how often a word $w$ and a context word $c$ appear together relative to how often we would expect them to co-occur if they were independent.\n",
    "\n",
    "The PMI between a word $w$ and a context word $c$ is defined as:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{PMI}(w, c) &= \\log_2 \\frac{P(w, c)}{P(w)P(c)}\n",
    "\\end{align*}}\n",
    "$$\n",
    "where $P(w, c)$ is the probability of observing $w$ and $c$ in the same window, and $P(w)$ and $P(c)$ are the corresponding marginal probabilities. We'll estimate these probabilities from word-context co-occurrence counts in the corpus.\n",
    "\n",
    "First, let's build a word-context co-occurrence matrix using a window size of $m=2$ tokens on each side. We'll store these counts in the `cooccurrence_matrix::Array{Int64, 2}` variable, where rows correspond to target words and columns correspond to context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dcbbcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×30 Matrix{Int64}:\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  1  0  0\n",
       " 0  0  0  0  0  0  1  1  0  1  0  0  0     0  2  0  2  0  0  1  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  1  0  1  0  1  1     0  0  0  0  0  0  1  0  0  4  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     1  0  0  0  0  1  0  0  0  1  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  1     0  0  0  0  0  1  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  1  0  0  0  …  0  0  0  0  1  0  1  1  0  0  0  0\n",
       " 0  1  0  0  0  0  0  1  0  1  0  0  0     0  2  0  1  0  0  1  0  0  0  0  0\n",
       " 0  1  1  0  0  0  1  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  1  1  1  0  0  0  0  0  0  0  0\n",
       " 0  1  1  0  0  1  1  0  0  0  0  0  0     0  1  0  0  0  0  2  1  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  1  0  0  1  0  0  0  1  0  0  0\n",
       " 0  0  1  0  0  0  0  0  0  0  0  0  0     0  1  0  0  0  0  0  0  0  1  0  0\n",
       " 0  0  1  0  1  0  0  0  0  0  0  0  0     1  1  0  0  0  1  0  0  0  1  0  0\n",
       " ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n",
       " 0  0  0  1  0  0  0  0  0  0  0  0  1     0  0  0  0  0  1  0  0  0  1  0  0\n",
       " 0  2  0  0  0  0  2  0  1  1  1  1  1     0  0  1  5  1  0  0  1  2  0  0  0\n",
       " 0  0  0  0  0  0  0  0  1  0  0  0  0  …  0  1  0  2  0  0  0  0  2  0  0  0\n",
       " 0  2  0  0  0  0  1  0  1  0  0  0  0     0  5  2  0  0  0  1  0  2  0  0  0\n",
       " 0  0  0  0  0  1  0  0  0  0  1  0  0     0  1  0  0  0  0  0  1  0  0  0  0\n",
       " 0  0  0  1  1  0  0  0  0  0  0  0  1     1  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  1  1  0  0  1  1  0  0  2  0  0  0     0  0  0  1  0  0  0  0  0  1  0  0\n",
       " 0  0  0  0  0  1  0  0  0  1  0  0  0  …  0  1  0  0  1  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  1  0  0     0  2  2  2  0  0  0  0  0  0  0  0\n",
       " 1  0  4  1  0  0  0  0  0  0  0  1  1     1  0  0  0  0  0  1  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    window_size = 2; # number of tokens on each side\n",
    "    vocab_size = length(vocabulary); # size of the vocabulary\n",
    "    cooccurrence_matrix = zeros(Int64, vocab_size, vocab_size); # initialize the co-occurrence matrix\n",
    "\n",
    "    # populate the co-occurrence matrix -\n",
    "    for sentence in sentences\n",
    "        augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "        words = split(lowercase(augmented_sentence)) .|> String;\n",
    "        word_indices = [get(vocabulary, word, vocabulary[\"<unk>\"]) for word in words]; # wow! nice one-liner to get indices with <unk> fallback\n",
    "\n",
    "        for i ∈ eachindex(word_indices)\n",
    "            target_index = word_indices[i];\n",
    "            left = max(1, i - window_size); # ensure we don't go out of bounds to the left\n",
    "            right = min(length(word_indices), i + window_size); # ensure we don't go out of bounds to the right\n",
    "            \n",
    "            \n",
    "            for j ∈ left:right\n",
    "                if j == i\n",
    "                    continue\n",
    "                end\n",
    "                context_index = word_indices[j];\n",
    "                cooccurrence_matrix[target_index, context_index] += 1;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the co-occurrence matrix\n",
    "    cooccurrence_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047983c2",
   "metadata": {},
   "source": [
    "Next, we'll convert the co-occurrence counts to probabilities and compute PMI. We'll store the PMI values in the `PMI_matrix::Array{Float64, 2}` variable, and the Positive PMI values in the `PPMI_matrix::Array{Float64, 2}` variable by zeroing out negative entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb962904",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_matrix, PPMI_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    vocab_size = size(cooccurrence_matrix, 1); # size of the vocabulary\n",
    "    total_pairs = sum(cooccurrence_matrix); # total number of word-context pairs observed\n",
    "\n",
    "    # compute probabilities - all from the co-occurrence sample space\n",
    "    P_wc = cooccurrence_matrix / total_pairs; # P(w,c) = joint probability of co-occurrence\n",
    "    \n",
    "    # Marginal probabilities from co-occurrence matrix\n",
    "    # P(w) = sum over all contexts of P(w,c) = probability that word w appears as target\n",
    "    # P(c) = sum over all targets of P(w,c) = probability that word c appears as context\n",
    "    P_w = vec(sum(P_wc, dims=2)); # sum across columns (all contexts for each target word)\n",
    "    P_c = vec(sum(P_wc, dims=1)); # sum across rows (all targets for each context word)\n",
    "\n",
    "    # compute PMI -\n",
    "    PMI_matrix = fill(-Inf, vocab_size, vocab_size); # initialize PMI matrix\n",
    "    for i in 1:vocab_size\n",
    "        for j in 1:vocab_size\n",
    "            p_wc = P_wc[i, j];\n",
    "            p_w = P_w[i];\n",
    "            p_c = P_c[j];\n",
    "            if p_wc == 0 || p_w == 0 || p_c == 0\n",
    "                continue\n",
    "            end\n",
    "            PMI_matrix[i, j] = log2(p_wc / (p_w * p_c));\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # compute PPMI -\n",
    "    PPMI_matrix = max.(PMI_matrix, 0.0);\n",
    "\n",
    "    # return PMI and PPMI matrices\n",
    "    (PMI_matrix, PPMI_matrix)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654266b",
   "metadata": {},
   "source": [
    "Now we can inspect the PMI or PPMI matrices to see which word pairs co-occur more than expected in this small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b6e2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×30 Matrix{Float64}:\n",
       " 0.0      0.0      0.0      0.0      0.0      …  0.0       2.53051  0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       2.53051  0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       2.53051  0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0      …  0.0       0.0      0.0  0.0\n",
       " 0.0      1.70044  0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      2.70044  2.11548  0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      1.70044  1.11548  0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0      …  2.11548   0.0      0.0  0.0\n",
       " 0.0      0.0      2.11548  0.0      0.0         0.0       2.11548  0.0  0.0\n",
       " 0.0      0.0      1.11548  0.0      2.70044     0.0       1.11548  0.0  0.0\n",
       " ⋮                                            ⋱                          \n",
       " 0.0      0.0      0.0      4.11548  0.0         0.0       2.11548  0.0  0.0\n",
       " 0.0      1.11548  0.0      0.0      0.0         0.530515  0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0      …  2.11548   0.0      0.0  0.0\n",
       " 0.0      1.53051  0.0      0.0      0.0         0.945552  0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      4.11548  3.70044     0.0       0.0      0.0  0.0\n",
       " 0.0      1.70044  1.11548  0.0      0.0         0.0       1.11548  0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0      …  0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 2.53051  0.0      2.53051  2.53051  0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0\n",
       " 0.0      0.0      0.0      0.0      0.0         0.0       0.0      0.0  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6d0a4",
   "metadata": {},
   "source": [
    "### Understanding Window Size and Co-occurrence\n",
    "Two words \"co-occur\" in PMI if they appear within a specified window size of each other, not just in the same sentence.\n",
    "\n",
    "> __Important:__ With `window_size = 2`, a word only \"sees\" the 2 tokens immediately before and 2 tokens immediately after it. Words that are more than 2 positions apart do NOT co-occur, even if they're in the same sentence.\n",
    ">\n",
    "> For example, in \"I love coding machine learning in Julia\", the words \"love\" and \"julia\" are 5 positions apart, so they don't co-occur with `window_size = 2`.\n",
    "\n",
    "Let's test this by comparing two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b262e084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: \"love\" and \"julia\":\n",
      "  Co-occurrence count: 0\n",
      "  PPMI value: 0.0\n",
      "  Note: These words appear in sentence 4 but are 5 positions apart (beyond window_size=2)\n",
      "\n",
      "Example 2: \"machine\" and \"learning\":\n",
      "  Co-occurrence count: 5\n",
      "  PPMI value: 1.267480310864986\n",
      "  Note: These words appear together frequently within window_size=2\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # Example 1: Words that are FAR apart in a sentence (window_size = 2)\n",
    "    word_1 = \"love\" |> lowercase;\n",
    "    word_2 = \"julia\" |> lowercase;\n",
    "    index_1 = vocabulary[word_1];\n",
    "    index_2 = vocabulary[word_2];\n",
    "    \n",
    "    # Check the co-occurrence count\n",
    "    cooccurrence_count = cooccurrence_matrix[index_1, index_2];\n",
    "    ppmi_value = PPMI_matrix[index_1, index_2];\n",
    "    \n",
    "    println(\"Example 1: \\\"$word_1\\\" and \\\"$word_2\\\":\");\n",
    "    println(\"  Co-occurrence count: $cooccurrence_count\");\n",
    "    println(\"  PPMI value: $ppmi_value\");\n",
    "    println(\"  Note: These words appear in sentence 4 but are 5 positions apart (beyond window_size=2)\\n\");\n",
    "    \n",
    "    # Example 2: Words that ARE within the window\n",
    "    word_3 = \"machine\" |> lowercase;\n",
    "    word_4 = \"learning\" |> lowercase;\n",
    "    index_3 = vocabulary[word_3];\n",
    "    index_4 = vocabulary[word_4];\n",
    "    \n",
    "    cooccurrence_count_2 = cooccurrence_matrix[index_3, index_4];\n",
    "    ppmi_value_2 = PPMI_matrix[index_3, index_4];\n",
    "    \n",
    "    println(\"Example 2: \\\"$word_3\\\" and \\\"$word_4\\\":\");\n",
    "    println(\"  Co-occurrence count: $cooccurrence_count_2\");\n",
    "    println(\"  PPMI value: $ppmi_value_2\");\n",
    "    println(\"  Note: These words appear together frequently within window_size=2\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143dabc",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee8645",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this example, we implemented and explored three fundamental text embedding techniques: Bag of Words, TF-IDF, and PMI.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **Bag of Words limitations:** BoW provides a simple frequency-based representation but suffers from sparsity, high dimensionality, and a lack of context or semantic meaning.\n",
    "> * **TF-IDF re-weighting:** TF-IDF improves upon raw counts by penalizing common words and emphasizing terms that are distinctive to specific documents.\n",
    "> * **PMI for associations:** PMI and PPMI quantify the statistical association between words based on co-occurrence, capturing semantic relationships that simple counts miss.\n",
    "\n",
    "These methods form the foundation for more advanced natural language processing tasks.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.3",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
