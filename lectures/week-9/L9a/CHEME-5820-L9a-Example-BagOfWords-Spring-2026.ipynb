{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b73d96",
   "metadata": {},
   "source": [
    "# Example: Bag of Words, TF-IDF and PMI\n",
    "In this example, we'll play around with simple collections of text data and explore how to create basic text embeddings using the Bag of Words model, Term Frequency-Inverse Document Frequency (TF-IDF), and Pointwise Mutual Information (PMI).\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this example, you should be able to:\n",
    ">\n",
    "> Three learning objectives go here.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9a118",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants.\n",
    "\n",
    "> The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a8b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5046110",
   "metadata": {},
   "source": [
    "### Data\n",
    "Here, let's construct a small corpus of simple sentences to work with in the `sentences::Array{String,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98193ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = let\n",
    "\n",
    "    # initialize -\n",
    "    sentences_array = Array{String,1}(); # initialize an array of sentence strings\n",
    "\n",
    "    # add sentences -\n",
    "    push!(sentences_array, \"I love machine learning and data science .\");\n",
    "    push!(sentences_array, \"Machine learning is fun .\"); # the second sentence is close to the third sentence\n",
    "    push!(sentences_array, \"Machine learning is great .\"); # expect similarity b/w 2nd and 3rd sentences\n",
    "    push!(sentences_array, \"I love coding in Julia !\");\n",
    "    push!(sentences_array, \"Julia is a great programming language ?\");\n",
    "    push!(sentences_array, \"I enjoy learning new things about data science , machine learning , and artificial intelligence .\");\n",
    "    \n",
    "    # return the array\n",
    "    sentences_array\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a3828",
   "metadata": {},
   "source": [
    "Next, we'll preprocess the data in the `sentences::Array{String,1}` variable by tokenizing the sentences into words, and converting them to lowercase. This, along with our control tokens, will be our vocabulary model.\n",
    "\n",
    "We'll store the vocabulary in the `vocabulary::Dict{String, Int64}` variable, where the keys are the unique words and the values are their corresponding indices, and the inverse vocabulary in the `inverse_vocabulary::Dict{Int64, String}` variable, where the keys are the indices and the values are the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047e1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict(\"!\" => 1, \"is\" => 17, \"enjoy\" => 11, \"data\" => 10, \"language\" => 19, \"coding\" => 9, \"science\" => 25, \"<bos>\" => 27, \"a\" => 5, \"and\" => 7…), Dict(5 => \"a\", 16 => \"intelligence\", 20 => \"learning\", 12 => \"fun\", 24 => \"programming\", 28 => \"<eos>\", 8 => \"artificial\", 17 => \"is\", 30 => \"<unk>\", 1 => \"!\"…))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, inverse_vocabulary = let\n",
    "\n",
    "    # initialize -\n",
    "    vocabulary = Dict{String, Int64}(); # initialize the vocabulary dictionary\n",
    "    inverse_vocabulary = Dict{Int64, String}(); # initialize the inverse vocabulary dictionary\n",
    "    index = 1; # initialize the index counter\n",
    "\n",
    "    # control tokens -\n",
    "    control_tokens = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]; # define control tokens\n",
    "\n",
    "    # tmp variables -\n",
    "    words = Set{String}(); # temporary array to hold words\n",
    "    for sentence in sentences\n",
    "        tmp = split(lowercase(sentence)); # convert to lowercase and split by whitespace\n",
    "        push!(words, tmp...); # add words to the set\n",
    "    end\n",
    "    words_array = collect(words) |> sort; # convert set to array, and sort it\n",
    "\n",
    "    # append the control tokens to the words array\n",
    "    words_array = vcat(words_array, control_tokens);\n",
    "    for word in words_array\n",
    "        vocabulary[word] = index;\n",
    "        inverse_vocabulary[index] = word;\n",
    "        index += 1;\n",
    "    end\n",
    "\n",
    "    # return the vocabulary and inverse vocabulary\n",
    "    (vocabulary, inverse_vocabulary)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54542876",
   "metadata": {},
   "source": [
    "What's in the `vocabulary::Dict{String, Int64}` and `inverse_vocabulary::Dict{Int64, String}` variables? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5166d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Int64} with 30 entries:\n",
       "  \"!\"            => 1\n",
       "  \"is\"           => 17\n",
       "  \"enjoy\"        => 11\n",
       "  \"data\"         => 10\n",
       "  \"language\"     => 19\n",
       "  \"coding\"       => 9\n",
       "  \"science\"      => 25\n",
       "  \"<bos>\"        => 27\n",
       "  \"a\"            => 5\n",
       "  \"and\"          => 7\n",
       "  \",\"            => 2\n",
       "  \"programming\"  => 24\n",
       "  \"love\"         => 21\n",
       "  \"?\"            => 4\n",
       "  \".\"            => 3\n",
       "  \"in\"           => 15\n",
       "  \"i\"            => 14\n",
       "  \"<unk>\"        => 30\n",
       "  \"about\"        => 6\n",
       "  \"<pad>\"        => 29\n",
       "  \"machine\"      => 22\n",
       "  \"artificial\"   => 8\n",
       "  \"learning\"     => 20\n",
       "  \"intelligence\" => 16\n",
       "  \"great\"        => 13\n",
       "  ⋮              => ⋮"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb9a93",
   "metadata": {},
   "source": [
    "### Helper Implementations\n",
    "Fill me in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8beedf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashing_vectorizer (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hashing_vectorizer(features::Array{String,1}; length::Int64 = 10)::Array{Int64,1}\n",
    "\n",
    "    # initalize -\n",
    "    new_hash_vector = zeros(Int,length);\n",
    "    for i ∈ eachindex(features)\n",
    "        feature = features[i]; # get feature\n",
    "        h = hash(feature);\n",
    "        j = mod(h,length); # this gives us an index in 0:(length-1)\n",
    "        \n",
    "        if (j == 0)\n",
    "            j = length;\n",
    "        end\n",
    "        new_hash_vector[j] += 1;\n",
    "    end\n",
    "   \n",
    "    new_hash_vector; # return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a69a4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6e323",
   "metadata": {},
   "source": [
    "## Task 1: Bag of Words Representations\n",
    "In this task, we'll create a Bag of Words representation for our corpus of sentences. \n",
    "\n",
    "> __Bag of Words Model__\n",
    ">\n",
    "> The Bag of Words (BoW) model is a technique for text embedding. As the name suggests, we represent a text (such as a sentence or a document) as a \"bag\" (multiset) of its words, disregarding grammar and word order but keeping multiplicity. Given a vocabulary of unique words, each text is represented as a vector where each dimension corresponds to a word in the vocabulary, and the value in that dimension indicates the frequency of that word in the text.\n",
    "\n",
    "Let's compute the Bag of Words representation for each sentence in our corpus and store the results in the `bow_matrix::Array{Int64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×30 Matrix{Int64}:\n",
       " 0  0  1  0  0  0  1  0  0  1  0  0  0  …  0  1  1  1  0  0  1  0  1  1  0  0\n",
       " 0  0  1  0  0  0  0  0  0  0  0  1  0     0  1  0  1  0  0  0  0  1  1  0  0\n",
       " 1  0  0  0  0  0  0  0  1  0  0  0  0     0  0  1  0  0  0  0  0  1  1  0  0\n",
       " 0  0  0  1  1  0  0  0  0  0  0  0  1     1  0  0  0  0  1  0  0  1  1  0  0\n",
       " 0  2  1  0  0  1  1  1  0  1  1  0  0     0  2  0  1  1  0  1  1  1  1  0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = length(sentences); # number of sentences\n",
    "    vocab_size = length(vocabulary); # size of the vocabulary\n",
    "    bow_matrix = zeros(Int64, num_sentences, vocab_size); # initialize the Bag of Words matrix\n",
    "\n",
    "    # populate the Bag of Words matrix -\n",
    "    for (i, sentence) in enumerate(sentences)\n",
    "\n",
    "        # add the <BOS> ... <EOS> token wrappers\n",
    "        augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "        words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "        for word in words\n",
    "            if haskey(vocabulary, word)\n",
    "                index = vocabulary[word];\n",
    "                bow_matrix[i, index] += 1; # increment the count for the word\n",
    "            else\n",
    "                unk_index = vocabulary[\"<unk>\"];\n",
    "                bow_matrix[i, unk_index] += 1; # increment the count for unknown words\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the Bag of Words matrix\n",
    "    bow_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1486fe",
   "metadata": {},
   "source": [
    "### What is wrong with this representation? \n",
    "Fill me in later.\n",
    "\n",
    "Let's consider an altenative representation. Instead of maintaining a vocabulary dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying [a hash function `h(...)`](https://docs.julialang.org/en/v1/base/base/#Base.hash) to the features (e.g., words), then using the hash values directly as feature indices and updating the resulting vector at those indices. \n",
    "\n",
    "We've implemented a simple version of a hashing vectorizer in the `hashing_vectorizer(...)` function. We pass in a list of words (from a sentence) and the desired length of the output vector, and the function returns a vector of the specified length with counts of the hashed words.\n",
    "\n",
    "We save this output in the `vectorized_sentence::Array{Int64,1}` variable. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ccd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30-element Vector{Int64}:\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_sentence = let\n",
    "\n",
    "    # initialize -\n",
    "    i = 1; # index of sentence to hash\n",
    "    sentence = sentences[i];\n",
    "    augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "    words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "    tv = hashing_vectorizer(words, length = length(vocabulary));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80325667",
   "metadata": {},
   "source": [
    "You can change the sentence index `i` in the code block above to see how different sentences are represented using this hashing vectorizer. Futher, you can modify the `length` parameter to see how the size of the output vector affects the representation.\n",
    "\n",
    "Play around with different sentences and vector lengths to see how the hashing vectorizer performs! We'll save this output in the `alternative_vectorized_sentence::Array{Int64,1}` variable. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f382f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Int64}:\n",
       " 0\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_vectorized_sentence = let\n",
    "\n",
    "    # initialize -\n",
    "    i = 1; # index of sentence to hash\n",
    "    sentence = sentences[i];\n",
    "    desired_sentence_length = 10; # desired length of the output vector\n",
    "    augmented_sentence = \"<bos> \" * sentence * \" <eos>\";\n",
    "    words = split(lowercase(augmented_sentence)) .|> String; # convert to lowercase and split by whitespace\n",
    "    tv = hashing_vectorizer(words, length = desired_sentence_length);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df41bf4",
   "metadata": {},
   "source": [
    "## Task 2: TF-IDF Representations\n",
    "In this task, we'll compute the Term Frequency-Inverse Document Frequency (TF-IDF) representation for our corpus of sentences. The TF-IDF score for a term $t$ in a document $d$ is given by the product of two terms:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF}(t, d) &= \\text{tf}(t, d) \\cdot \\text{idf}(t, \\mathcal{D})\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "The __Term Frequency__ ($\\text{tf}$) is the raw count of term $t$ in document $d$, often normalized by the total number of words in $d$. The __Inverse Document Frequency__ ($\\text{idf}$) measures how much the term is tied to a subset of documents. It is calculated as:\n",
    "    $$ \\text{idf}(t, \\mathcal{D}) = \\log \\left( \\frac{N}{|\\{d \\in \\mathcal{D} : t \\in d\\}|} \\right) $$\n",
    "where $N$ is the total number of documents in the corpus $\\mathcal{D}$, and the denominator is the number of documents where the term $t$ appears. In practice (especially for small corpora), we often use a smoothed IDF to avoid division-by-zero and to reduce extreme values; we'll use that smoothed form below.\n",
    "\n",
    "First, let's compute the TF values for each term in our vocabulary for each sentence in the corpus. We'll store these values in the `TF_matrix::Array{Float64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85262330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×30 Matrix{Float64}:\n",
       " 0.0    0.0       0.1        0.0       …  0.1        0.1        0.0  0.0\n",
       " 0.0    0.0       0.142857   0.0          0.142857   0.142857   0.0  0.0\n",
       " 0.125  0.0       0.0        0.0          0.125      0.125      0.0  0.0\n",
       " 0.0    0.0       0.0        0.111111     0.111111   0.111111   0.0  0.0\n",
       " 0.0    0.111111  0.0555556  0.0          0.0555556  0.0555556  0.0  0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = size(bow_matrix, 1); # number of sentences\n",
    "    vocab_size = size(bow_matrix, 2); # size of the vocabulary\n",
    "    TF_matrix = zeros(Float64, num_sentences, vocab_size); # initialize the TF matrix\n",
    "\n",
    "    # populate the TF matrix -\n",
    "    for i in 1:num_sentences\n",
    "        total_terms = sum(bow_matrix[i, :]);\n",
    "        if total_terms == 0\n",
    "            continue\n",
    "        end\n",
    "        for j in 1:vocab_size\n",
    "            TF_matrix[i, j] = bow_matrix[i, j] / total_terms;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the TF matrix\n",
    "    TF_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7bff3",
   "metadata": {},
   "source": [
    "Next, let's compute the IDF values for each term in our vocabulary across the entire corpus. We'll store these values in the `IDF_values_dictionary::Dict{String, Float64}` variable, where the keys are the unique words and the values are their corresponding IDF scores.\n",
    "\n",
    "To keep IDF values finite in small corpora (and to avoid division-by-zero when a term appears in zero documents), we'll use a smoothed IDF:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{idf}(t, \\mathcal{D}) &= \\log \\left( \\frac{N + 1}{|\\{d \\in \\mathcal{D} : t \\in d\\}| + 1} \\right)\n",
    "\\end{align*}}\n",
    "$$\n",
    "where $N$ is the total number of documents in the corpus $\\mathcal{D}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d4588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Float64} with 30 entries:\n",
       "  \"!\"            => 1.60944\n",
       "  \"is\"           => 0.916291\n",
       "  \"enjoy\"        => 1.60944\n",
       "  \"data\"         => 0.916291\n",
       "  \"language\"     => 1.60944\n",
       "  \"coding\"       => 1.60944\n",
       "  \"science\"      => 0.916291\n",
       "  \"<bos>\"        => 0.0\n",
       "  \"a\"            => 1.60944\n",
       "  \"and\"          => 0.916291\n",
       "  \",\"            => 1.60944\n",
       "  \"programming\"  => 1.60944\n",
       "  \"love\"         => 0.916291\n",
       "  \"?\"            => 1.60944\n",
       "  \".\"            => 0.510826\n",
       "  \"in\"           => 1.60944\n",
       "  \"i\"            => 0.510826\n",
       "  \"<unk>\"        => 0.0\n",
       "  \"about\"        => 1.60944\n",
       "  \"<pad>\"        => 0.0\n",
       "  \"machine\"      => 0.510826\n",
       "  \"artificial\"   => 1.60944\n",
       "  \"learning\"     => 0.510826\n",
       "  \"intelligence\" => 1.60944\n",
       "  \"great\"        => 1.60944\n",
       "  ⋮              => ⋮"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_values_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = length(sentences); # number of sentences\n",
    "    IDF_values_dictionary = Dict{String, Float64}(); # initialize dictionary of IDF values\n",
    "\n",
    "    # compute smoothed IDF for each term in the vocabulary -\n",
    "    for (word, index) in vocabulary\n",
    "        doc_frequency = sum(bow_matrix[:, index] .> 0);\n",
    "        IDF_values_dictionary[word] = log((num_sentences + 1) / (doc_frequency + 1));\n",
    "    end\n",
    "\n",
    "    # return dictionary\n",
    "    IDF_values_dictionary\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1c5c2",
   "metadata": {},
   "source": [
    "Finally, we can compute the TF-IDF representation for each sentence in our corpus by multiplying the TF values with the corresponding IDF values. We'll store the resulting TF-IDF representations in the `TFIDF_matrix::Array{Float64, 2}` variable, where each row corresponds to a sentence and each column corresponds to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8baf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×30 Matrix{Float64}:\n",
       " 0.0      0.0       0.0510826  0.0       …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0      0.0       0.0729751  0.0          0.0        0.0  0.0  0.0  0.0\n",
       " 0.20118  0.0       0.0        0.0          0.0        0.0  0.0  0.0  0.0\n",
       " 0.0      0.0       0.0        0.178826     0.0        0.0  0.0  0.0  0.0\n",
       " 0.0      0.178826  0.0283792  0.0          0.0894132  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_matrix = let\n",
    "\n",
    "    # initialize -\n",
    "    num_sentences = size(bow_matrix, 1); # number of sentences\n",
    "    vocab_size = size(bow_matrix, 2); # size of the vocabulary\n",
    "    TFIDF_matrix = zeros(Float64, num_sentences, vocab_size); # initialize the TF-IDF matrix\n",
    "\n",
    "    # populate the TF-IDF matrix -\n",
    "    for i in 1:num_sentences\n",
    "        for j in 1:vocab_size\n",
    "            word = inverse_vocabulary[j];\n",
    "            idf_value = IDF_values_dictionary[word];\n",
    "            TFIDF_matrix[i, j] = TF_matrix[i, j] * idf_value;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the TF-IDF matrix\n",
    "    TFIDF_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225f2b9",
   "metadata": {},
   "source": [
    "So what does the TF-IDF representation tell us about our sentences?\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced0d04",
   "metadata": {},
   "source": [
    "## Task 3: PMI Representations\n",
    "Fill me in later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee8645",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One concise, direct summary sentence goes here.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> Three key takeaways go here.\n",
    "\n",
    "One concise, direct concluding sentence goes here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.3",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
