{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L3c: Logistic Regression and Regularization\n",
    "In this lecture, we will explore logistic regression, a technique for binary classification tasks. We will also discuss the concept of regularization, which helps prevent overfitting in our models.\n",
    "\n",
    "> __Learning Objectives__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> Three learning objectives here.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba48551",
   "metadata": {},
   "source": [
    "## Example\n",
    "Today, we will use the following examples to illustrate key concepts:\n",
    " \n",
    "> [▶ Logistic classification of a banknote dataset](CHEME-5820-L3c-Example-LogisticRegression-GD-Spring-2026.ipynb). In this example, we'll use logistic regression to classify authentic and inauthentic banknotes based on features extracted from images of the banknotes. We'll train a logistic regression model using gradient descent and evaluate its performance using the confusion matrix.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4449c",
   "metadata": {},
   "source": [
    "## Logistic Regression: Cross-entropy loss\n",
    "Suppose we view our two–class labels $y\\in\\{-1,1\\}$ as _states_ in a Boltzmann distribution conditioned on the input $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{m+1}$ (the original feature vector with a `1` as the last element to account for a bias). Then for any state $y$ with energy $E(y,\\hat{\\mathbf{x}})$ at (unit) temperature, the conditional probability of observing the label $y\\in\\left\\{-1,+1\\right\\}$ given the feature vector $\\hat{\\mathbf{x}}$ can be represented as\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y\\mid \\hat{\\mathbf{x}})\n",
    "=\\frac{\\exp\\bigl(-E(y,\\hat{\\mathbf{x}})\\bigr)}\n",
    "      {\\underbrace{\\sum_{y' \\in\\{-1,1\\}} \\exp\\bigl(-E(y',\\hat{\\mathbf{x}})\\bigr)}_{Z(\\hat{\\mathbf{x}})}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "For the energy function, we can use a linear model of the form:\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(y,\\hat{\\mathbf{x}})\\;=\\;-\\,y\\;\\bigl(\\hat{\\mathbf{x}}^{\\top}\\theta \\bigr).\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\theta\\in\\mathbb{R}^{p}$ is a vector of __unknown__ parameters (weights plus bias) that we want to learn. When $y=+1$, the energy $E(1,\\hat{\\mathbf{x}})=-\\hat{\\mathbf{x}}^{\\top}\\theta$ is *lower* (more probable) if $\\hat{\\mathbf{x}}^{\\top}\\theta$ is large. On the other hand, when $y=-1$, the energy $E(-1,\\hat{\\mathbf{x}})=+\\hat{\\mathbf{x}}^{\\top}\\theta$, so $y=-1$ is favored when $\\hat{\\mathbf{x}}^{\\top}\\theta$ is very negative.\n",
    "\n",
    "Let's substitute the energy function into the conditional probability expression and do some algebra:\n",
    "$$\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y\\mid \\hat{\\mathbf{x}})\n",
    "& =\\frac{\\exp\\bigl(-E(y,\\hat{\\mathbf{x}})\\bigr)}\n",
    "      {\\underbrace{\\sum_{y' \\in\\{-1,1\\}} \\exp\\bigl(-E(y',\\hat{\\mathbf{x}})\\bigr)}_{Z(\\hat{\\mathbf{x}})}}\\\\\n",
    "&=\\frac{\\exp\\bigl(y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\n",
    "      {\\exp\\bigl(\\hat{\\mathbf{x}}^{\\top}\\theta\\bigr) + \\exp\\bigl(-\\hat{\\mathbf{x}}^{\\top}\\theta\\bigr)}\\quad\\Longrightarrow\\;{\\text{substituting } z = \\hat{\\mathbf{x}}^{\\top}\\theta}\\\\\n",
    "& = \\frac{\\exp\\bigl(yz\\bigr)}\n",
    "      {\\exp\\bigl(z\\bigr) + \\exp\\bigl(-z\\bigr)}\\quad\\Longrightarrow\\;{\\text{factor out}\\; \\exp(yz)\\;\\text{from denominator}}\\\\\n",
    "& = \\frac{\\exp\\bigl(yz\\bigr)}\n",
    "      {\\exp\\bigl(yz\\bigr)\\left(\\exp\\bigl((1-y)z\\bigr) + \\exp\\bigl(-(1+y)z\\bigr)\\right)}\\quad\\Longrightarrow\\;\\text{cancel}\\;\\exp(yz)\\\\\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl((1-y)z\\bigr) + \\exp\\bigl(-(1+y)z\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This expression is the probability of observing the label $y$ given the feature vector $\\hat{\\mathbf{x}}$ and the parameters $\\theta$. Let's look at the case when $y=+1$ and $y=-1$:\n",
    "\n",
    "> __Cases:__\n",
    ">\n",
    "> When $y=+1$, we have:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y = +1\\mid \\hat{\\mathbf{x}})\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl(0\\bigr) + \\exp\\bigl(-2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\\\\n",
    "& = \\frac{1}\n",
    "      {1 + \\exp\\bigl(-2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "> \n",
    "> When $y=-1$, we have:\n",
    "> $$\\begin{align*}\n",
    "P_{\\theta}(y = -1\\mid \\hat{\\mathbf{x}})\n",
    "& = \\frac{1}\n",
    "      {\\exp\\bigl(2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr) + \\exp\\bigl(0\\bigr)}\\\\\n",
    "& = \\frac{1}\n",
    "      {1+\\exp\\bigl(2\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "> Putting this all together, we can write the conditional probability of observing the label $y$ given the feature vector $\\hat{\\mathbf{x}}$ and the parameters $\\theta$ as:\n",
    "> $$\\begin{align*}\n",
    "P_{\\theta}(y\\mid \\hat{\\mathbf{x}}) & = \\frac{1}{1+\\exp\\bigl(-2y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)}\\quad\\Longrightarrow\\;\\text{Logistic function!}\\\\\n",
    "& = \\sigma\\bigl(2y\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)\\bigr)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "### Parameter Estimation\n",
    "Of course, we want to learn the parameters $\\theta$ so that we maximize the log likelihood (or minimize the negative log-likelihood) of the observed labels given the feature vectors. The likelihood function is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) & = \\prod_{i=1}^{n} P_{\\theta}(y_{i}\\mid \\hat{\\mathbf{x}}_{i})\\\\\n",
    "& = \\prod_{i=1}^{n} \\frac{1}{1+\\exp\\bigl(-2y_{i}\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)}\\quad\\Longrightarrow\\;\\text{Product is $\\textbf{hard}$ to optimize! Take the $\\log$}\\\\\n",
    "\\log\\mathcal{L}(\\theta) & = -\\sum_{i=1}^n \\log\\!\\bigl(1+\\exp\\bigl(-2y_i\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)\\bigr)\\\\\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "We can use gradient descent to minimize the negative log-likelihood (also known as the cross-entropy loss function):\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "J(\\theta) & = -\\log\\mathcal{L}(\\theta)\\\\\n",
    "& = \\sum_{i=1}^n \\log\\!\\bigl(1+\\exp\\bigl(-2y_i\\,\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)\\bigr)\\bigr)\\quad\\blacksquare\\\\\n",
    "\\end{align*}}\n",
    "$$      \n",
    "This will give us the optimal parameters $\\theta$ for our logistic regression model:\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} J(\\theta)\n",
    "$$\n",
    "Ok, let's give this a try with an example.\n",
    "\n",
    "> __Example__\n",
    ">\n",
    "> [▶ Logistic classification of a banknote dataset](CHEME-5820-L3c-Example-LogisticRegression-GD-Spring-2026.ipynb). In this example, we'll use logistic regression to classify authentic and inauthentic banknotes based on features extracted from images of the banknotes. We'll train a logistic regression model using gradient descent and evaluate its performance using the confusion matrix.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df509c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One direct summary sentence goes here.\n",
    "\n",
    "> __Key Takeaways__\n",
    ">\n",
    "> Three key takeaways go here.\n",
    "\n",
    "One direct final concluding sentence goes here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
