{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L5c: Support Vector Machines (SVM)\n",
    "In this lecture, we will explore Support Vector Machines (SVM) and their kernelized versions. The SVM is a supervised learning algorithm used for classification and regression tasks. \n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you should be able to:\n",
    "> Three learning objectives go here. \n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d8183",
   "metadata": {},
   "source": [
    "## Theory: Support Vector Machine (SVM)\n",
    "Suppose, we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label.\n",
    "\n",
    "> __What is the goal of an SVM?__ \n",
    "> \n",
    "> The goal of an SVM is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or alternatively, the parameters of the model that we need to estimate.\n",
    "\n",
    "\n",
    "Support vector machines (SVMs) and other approaches, e.g., [the perceptron](https://en.wikipedia.org/wiki/Perceptron) differ primarily in their optimization objectives and training methods: while a [perceptron](https://en.wikipedia.org/wiki/Perceptron) can find _a hyperplane_ that separates classes, SVMs seek to find the _best hyperplane_ in the sense that the _margin_ between classes is maximized.\n",
    "\n",
    "There are (at least) two strategies that we could use to estimate the unknown parameters $\\theta \\in \\mathbb{R}^{p}$, depending upon if we know beforehand whether the dataset $\\mathcal{D}$ is linearly separable.\n",
    "\n",
    "Let's start with the case where we know that the dataset is linearly separable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826b403",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figs/Fig-SVM-Schematic.svg\" width=\"480\"/>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d8b69",
   "metadata": {},
   "source": [
    "### Hard margin case: Linearly separable data\n",
    "If the data is linearly separable, a hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}})$ exists that perfectly seperates the data. We can estimate the _best_ hyperplane by maximizing the _margin_, which is equivalent to minimizing the parameter vector $\\theta$. \n",
    "\n",
    "> __Hard Margin Problem__\n",
    "> \n",
    "> The __maximum hard margin problem__ for a support vector classifier is given by:\n",
    "> $$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1\\quad\\forall i\n",
    "\\end{align*}}\n",
    "> $$\n",
    "> where $\\theta\\in\\mathbb{R}^{p}$ denote the unknown parameters that we are trying to estimate, $\\hat{\\mathbf{x}}_{i}\\in\\mathbb{R}^{p}$ are the augmented (training) feature vectors, $y_{i}\\in\\{-1,1\\}$ are the class labels, and $p=m+1$ is the number of parameters, where $m$ is the number of features. The index $i$ runs over the training examples, i.e., one constraint per training example.\n",
    "\n",
    "__Do we solve the hard margin problem?__\n",
    "\n",
    "__Yes!__ We can solve the hard margin problem above using [quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming) (QP) techniques. In fact, many off-the-shelf optimization packages (e.g., [CVXOPT](https://cvxopt.org/), [OSQP](https://osqp.org/), [Gurobi](https://www.gurobi.com/), etc.) can solve QP problems efficiently.\n",
    "\n",
    "However, in practice, we rarely know beforehand whether the data is linearly separable. Therefore, we typically use a more general formulation of the SVM that can handle non-linearly separable data. Let's explore that next.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bee4ab",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figs/Fig-SVM-Schematic-Softmargin.svg\" width=\"480\"/>\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c10b5",
   "metadata": {},
   "source": [
    "## Soft margin case: Not linearly separable \n",
    "If the data is _not linearly separable_, then we know that a perfect $\\mathcal{H}(\\hat{\\mathbf{x}})$ will not exist, i.e., no hyperplane will separate the data without making at least one mistake. In this case, we can estimate the _best_ hyperplane possible by solving the maximum soft margin problem given by:\n",
    "\n",
    "> __Soft Margin Problem__\n",
    ">\n",
    "> The _maximum soft margin problem_ for a support vector classifier is given by:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1 - \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "> $$\n",
    "> where $\\xi_{i}$ is a _slack variable_, that quantifies the cost of a classification mistake, and $C>{0}$ is a user-adjustable parameter that controls the trade-off between maximizing the margin and minimizing the slack variables.\n",
    "\n",
    "__Values of the hyperparameter C__: If $C\\gg{1}$ the classifier will behave more like the maximum (hard) margin classifier, i.e., mistakes will be expensive, and the search will avoid making choices with mistakes. However, if $C\\ll{1}$, the classifier will allow more slack (mistakes), i.e., mistakes are cheap, so what's it matter!\n",
    "\n",
    "__Do we solve the soft margin problem?__\n",
    "\n",
    "__No!__ Typically, we don't solve the problem above directly; instead, we reformulate it as an _unconstrained_ problem [using a hinge-loss function](https://en.wikipedia.org/wiki/Hinge_loss):\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\min_{\\theta}\\left[\\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\max\\{0, 1 - y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right>\\}\\right]\n",
    "\\end{equation*}\n",
    "$$\n",
    "where the sum is computed over $n$ training examples. Yet again application of a penalty term to the objective function! Here, the penalty term is the _hinge loss function_, which penalizes misclassifications and points that are within the margin.\n",
    "\n",
    "__How do we solve this?__ We can solve this unconstrained optimization problem using a variety of techniques, including [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), or hueristic methods like [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) or [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing). Many off-the-shelf optimization packages can also solve this problem efficiently.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a5649",
   "metadata": {},
   "source": [
    "## Kernelized SVM\n",
    "In the previous sections, we assumed that the data could be separated by a linear hyperplane. However, in many real-world scenarios, the data may not be linearly separable in its original feature space. To address this, we can use the _kernel trick_ to implicitly map the data into a higher-dimensional space where it may be linearly separable.\n",
    "\n",
    "To apply the kernel trick to the soft margin classifier, we return to the (non-augmented) feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ and keep the bias term $b$ explicit. Let $\\phi(\\mathbf{x})$ denote the feature map associated with the kernel $K(\\mathbf{x}_{i},\\mathbf{x}_{j}) = \\left<\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j})\\right>$. The soft margin problem in feature space becomes:\n",
    "\n",
    "> __Kernelized Soft Margin Problem (primal)__\n",
    ">\n",
    "> The _kernelized soft margin problem_ can be written as:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta, b, \\xi}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left(\\left<\\phi(\\mathbf{x}_{i}),\\theta\\right> + b\\right) \\geq 1 - \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "> $$\n",
    "> where $\\theta$ is the normal vector in feature space, $b$ is the bias, and $\\xi_i$ are the slack variables.\n",
    "\n",
    "__Do we solve the primal?__ Almost never. In practice, we rarely have an explicit feature map $\\phi(\\cdot)$ (or it is too high-dimensional to construct), so solving the primal would defeat the purpose of the kernel trick. If an explicit, low-dimensional map is available and cheap to evaluate, then the primal is an option; otherwise we move to the dual.\n",
    "\n",
    "Solving the primal requires explicit access to $\\phi(\\cdot)$, which we want to avoid. Instead, we switch to the dual, where the inner products become kernel evaluations:\n",
    "\n",
    "> __Kernelized Soft Margin Problem (dual)__\n",
    ">\n",
    "> The dual (and the one we actually solve) is:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    \\max_{\\alpha}\\quad & \\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}K(\\mathbf{x}_{i},\\mathbf{x}_{j})\\\\\n",
    "    \\text{subject to}\\quad & 0 \\leq \\alpha_{i} \\leq C\\quad\\forall i\\\\\n",
    "    & \\sum_{i=1}^{n}\\alpha_{i}y_{i} = 0\n",
    "\\end{align*}\n",
    "> $$\n",
    "> where $\\alpha_i$ are the Lagrange multipliers (one per training example).\n",
    "\n",
    "__Decision function.__ After solving for $\\alpha$, we classify new points using:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    f(\\mathbf{x}) = \\sum_{i=1}^{n}\\alpha_{i}y_{i}K(\\mathbf{x}_{i},\\mathbf{x}) + b,\\qquad \\hat{y} = \\text{sign}\\{f(\\mathbf{x})\\}\n",
    "\\end{equation*}\n",
    "$$\n",
    "Only points with $\\alpha_{i} > 0$ appear in the sum; these are the _support vectors_.\n",
    "\n",
    "> __KKT reminder.__ Complementary slackness gives $\\alpha_i\\left[y_i f(\\mathbf{x}_i) - 1 + \\xi_i\\right]=0$ and $0\\le \\alpha_i \\le C$. Therefore, if a point satisfies $y_i f(\\mathbf{x}_i) > 1$ (strictly outside the margin) then $\\alpha_i=0$, while points on or inside the margin have $\\alpha_i>0$.\n",
    "\n",
    "For any support vector with $0 < \\alpha_{k} < C$, we can compute the bias term using:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    b = y_{k} - \\sum_{i=1}^{n}\\alpha_{i}y_{i}K(\\mathbf{x}_{i},\\mathbf{x}_{k})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "__Do we solve this?__ Yes! The dual is a quadratic program that depends only on the kernel matrix $K(\\mathbf{x}_{i},\\mathbf{x}_{j})$, so we can use the same QP machinery as before without ever computing $\\phi(\\cdot)$ explicitly.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e64425",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One direct, concice summary sentence goes here.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> THree key takeaways go here.\n",
    "\n",
    "One direct, concise concluding sentence goes here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
