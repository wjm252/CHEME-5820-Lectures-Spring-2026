{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f67472",
   "metadata": {},
   "source": [
    "# Derivation: Support Vector Regression (SVR)\n",
    "In the lecture notes, we discussed Support Vector Machines (SVM) for classification tasks. Now, let's explore how to adapt the SVM framework for regression tasks, leading to Support Vector Regression (SVR). \n",
    "\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector (with a trailing `1` for the bias) and $y_i \\in \\mathbb{R}$ is a scalar target.\n",
    "\n",
    "> __What is the goal of SVR?__\n",
    ">\n",
    "> The goal of SVR is to estimate a regression function $f(\\hat{\\mathbf{x}})=\\left<\\hat{\\mathbf{x}},\\theta\\right>$ that is _as flat as possible_ while keeping prediction errors within an $\\varepsilon$-insensitive tube.\n",
    "\n",
    "Instead of penalizing every error, SVR only penalizes deviations larger than $\\varepsilon$. This gives a robust regression model that ignores small errors and focuses on the largest deviations.\n",
    "\n",
    "> __Soft Margin SVR Problem__\n",
    ">\n",
    "> The $\\varepsilon$-insensitive soft margin problem is given by:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta, \\xi, \\xi^{*}}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}(\\xi_{i} + \\xi_{i}^{*})\\\\\n",
    "    \\text{subject to}\\quad & y_{i} - \\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\leq \\varepsilon + \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> - y_{i} \\leq \\varepsilon + \\xi_{i}^{*}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0,\\; \\xi_{i}^{*} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "> $$\n",
    "> where $\\xi_i$ and $\\xi_i^{*}$ are slack variables that measure violations above and below the $\\varepsilon$-tube, and $C>0$ controls the trade-off between flatness and error tolerance.\n",
    "\n",
    "__Values of $\\varepsilon$ and C__: Larger $\\varepsilon$ widens the tube (fewer support vectors, more bias), while smaller $\\varepsilon$ tightens the fit (more support vectors, less bias). The parameter $C$ controls how expensive tube violations are, just like in soft-margin classification.\n",
    "\n",
    "__Do we solve this?__ Yes, but we often use an equivalent unconstrained form with the $\\varepsilon$-insensitive loss:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\min_{\\theta}\\left[\\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\max\\{0, |y_i - \\left<\\hat{\\mathbf{x}}_{i},\\theta\\right>| - \\varepsilon\\}\\right]\n",
    "\\end{equation*}\n",
    "$$\n",
    "which is the direct regression analog of the hinge-loss formulation used for classification.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2573d65",
   "metadata": {},
   "source": [
    "## Kernelized SVR\n",
    "If the regression function is nonlinear, we use the kernel trick as before. Let $\\phi(\\hat{\\mathbf{x}})$ denote the feature map associated with a kernel $K(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}}_{j}) = \\left<\\phi(\\hat{\\mathbf{x}}_{i}),\\phi(\\hat{\\mathbf{x}}_{j})\\right>$.\n",
    "\n",
    "> __Kernelized SVR (dual)__\n",
    ">\n",
    "> The dual for the augmented-input formulation is:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    \\max_{\\alpha, \\alpha^{*}}\\quad & -\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\alpha_{i}-\\alpha_{i}^{*})(\\alpha_{j}-\\alpha_{j}^{*})K(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}}_{j})\\\\\n",
    "    & \\quad - \\varepsilon\\sum_{i=1}^{n}(\\alpha_{i}+\\alpha_{i}^{*}) + \\sum_{i=1}^{n}y_{i}(\\alpha_{i}-\\alpha_{i}^{*})\\\\\n",
    "    \\text{subject to}\\quad & 0 \\leq \\alpha_{i} \\leq C,\\; 0 \\leq \\alpha_{i}^{*} \\leq C\\quad\\forall i\n",
    "\\end{align*}\n",
    "> $$\n",
    "> where $\\alpha_i$ and $\\alpha_i^{*}$ are Lagrange multipliers (one pair per training example).\n",
    "\n",
    "__Note.__ Because we use augmented inputs, the bias term is regularized along with the weights, so there is no separate equality constraint. If $b$ is kept explicit, add the constraint $\\sum_{i=1}^{n}(\\alpha_i-\\alpha_i^{*})=0$ and include a separate bias term in the prediction.\n",
    "\n",
    "__Decision function.__ After solving the dual, the regressor is:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    f(\\hat{\\mathbf{x}}) = \\sum_{i=1}^{n}(\\alpha_{i}-\\alpha_{i}^{*})K(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}})\n",
    "\\end{equation*}\n",
    "$$\n",
    "Only points with $\\alpha_{i} > 0$ or $\\alpha_{i}^{*} > 0$ appear in the sum; these are the _support vectors_. Points strictly inside the $\\varepsilon$-tube have $\\alpha_i=\\alpha_i^{*}=0$.\n",
    "\n",
    "> __KKT reminder.__ Complementary slackness gives $\\alpha_i\\left[y_i - f(\\hat{\\mathbf{x}}_i) - \\varepsilon - \\xi_i\\right]=0$ and $\\alpha_i^{*}\\left[f(\\hat{\\mathbf{x}}_i) - y_i - \\varepsilon - \\xi_i^{*}\\right]=0$, so only points on or outside the tube can be support vectors.\n",
    "\n",
    "__Do we solve this?__ Yes! The dual is a quadratic program that depends only on the kernel matrix $K(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}}_{j})$.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
