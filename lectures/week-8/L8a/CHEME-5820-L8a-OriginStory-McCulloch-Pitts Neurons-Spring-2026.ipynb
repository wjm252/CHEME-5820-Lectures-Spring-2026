{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4c1668",
   "metadata": {},
   "source": [
    "# Origin story: McCulloch-Pitts Neurons\n",
    "In [their paper, McCulloch and Pitts (1943)](https://link.springer.com/article/10.1007/BF02478259) explored how the brain could produce highly complex patterns by using many [interconnected _basic cells (neurons)_](https://en.wikipedia.org/wiki/Biological_neuron_model). McCulloch and Pitts suggested a _highly simplified model_ of a neuron. Nevertheless, they made a foundational contribution to developing artificial neural networks that we find in wide use today. Let's look at the model of a neuron proposed by McCulloch and Pitts.\n",
    "\n",
    "Suppose we have a neuron that takes an input vector $\\mathbf{n}(t) = (n^{(t)}_1, n^{(t)}_2, \\ldots, n^{(t)}_{m})$, where each component $n_k\\in\\mathbf{n}$ is a binary value (`0` or `1`) representing the state of other predecessor neurons $n_1,n_2,\\ldots,n_m$ at time $t$. Then, the state of our neuron (say neuron $k$) at time $t+1$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "n_{k}(t+1) &= \\sigma\\left(\\sum_{j=1}^{m} w_{kj} n_j(t) - \\theta_k\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is an _activation function_ that maps the weighted sum of a vector of inputs to a scalar (binary) output. In the original paper, the state of neuron $k$ at time $t+1$, denoted as $n_k(t+1)\\in\\{0,1\\}$, where $w_{kj}$ is the weight of the connection from predecessor neuron $j$ to neuron $k$, and $\\theta_k$ is the threshold for neuron $k$. \n",
    "* __Activation function__: In the original McCulloch and Pitts model, the activation function $\\sigma$ is a step function, which means that the output of the neuron is `1` if the weighted sum of inputs exceeds the threshold $\\theta_k$, and `0` otherwise. In other words, the neuron \"fires\" (produces an output of `1`) if the total input to the neuron is greater than or equal to the threshold $\\theta_k$. This is a binary output, simplifying real biological neurons that can produce continuous outputs.\n",
    "* __Parameters__: The weights $w_{kj}\\in\\mathbb{R}$ and the threshold $\\theta_k\\in\\mathbb{R}$ are parameters of the neuron that determine its behavior. The weights can be positive or negative, representing the strength and direction of the influence of the input neurons on the output neuron. The threshold determines how much input the neuron needs to \"fire\" (i.e., produce an output of `1`).\n",
    "\n",
    "While the McCulloch-Pitts neuron model simplifies real biological neurons, it laid the groundwork for the development of more complex artificial neural networks. The key idea is that by combining many simple neurons in a network, we can create complex functions and learn to approximate any continuous function. This idea is at the heart of modern deep learning and neural networks. \n",
    "\n",
    "__Hmmmm__. These ideas formed the basis of many things that followed. Yes! The McCulloch-Pitts Neuron underpins [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron), [Hopfield networks](https://en.wikipedia.org/wiki/Hopfield_network), [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine), and eventually [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning).\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
