{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301524c-c75f-4769-9421-b31c4fc6f0e0",
   "metadata": {},
   "source": [
    "# Activity: Develop a Multiclass Artificial Neural Network Image Classifier\n",
    "In this activity, we'll develop a [Feed-Forward Neural Network (FNN)](https://en.wikipedia.org/wiki/Feedforward_neural_network) that classifies images of handwritten digits (0–9) from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). \n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> By the end of this example, you should be able to:\n",
    "> * **Build and train a multiclass feedforward neural network**: Construct an FNN using the Flux.jl library with multiple dense layers and activation functions for classifying images into 10-digit classes.\n",
    "> * **Prepare image data for neural network training**: Convert grayscale images from matrix format to vectorized Float32 input with one-hot encoded labels.\n",
    "> * **Evaluate model generalization on unseen data**: Compare classification accuracy on training and test datasets to assess overfitting and model performance.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cf348",
   "metadata": {},
   "source": [
    "## Background: Feedforward Neural Networks for Image Classification\n",
    "A feedforward neural network processes input through sequential layers of neurons, each applying a linear transformation followed by a nonlinear activation function. For image classification, the network learns to map pixel values to class probabilities.\n",
    "\n",
    "> __Why FNNs for image classification?__\n",
    ">\n",
    "> * __Universal approximation__: FNNs with sufficient hidden neurons can approximate any continuous function, making them suitable for learning complex mappings from images to labels.\n",
    "> * __Multiclass output__: The softmax activation in the output layer converts raw scores into a probability distribution over classes, enabling classification into one of multiple categories.\n",
    "> * __Supervised batch learning__: FNNs learn their parameters from a fixed set of labeled examples by minimizing a loss function, providing stable training on well-curated datasets.\n",
    "\n",
    "### Cross Entropy Loss Function\n",
    "Given training data $\\{(\\mathbf{x}_{i}, \\mathbf{y}_{i})\\}_{i=1}^{N}$ where $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ is a vectorized image and $\\mathbf{y}_{i}$ is the __one-hot encoded label__, the training minimizes the logit cross-entropy loss:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\underbrace{\\left(\\sum_{j=1}^{C} \\underbrace{y_{ij}}_{\\text{true}}\\cdot\\overbrace{\\log(p_{ij}(\\theta))}^{\\text{predicted}}\\right)}_{\\text{each example}}\n",
    "$$\n",
    "where $C$ is the number of classes and $p_{ij}(\\theta)$ is the predicted probability that example $i$ belongs to class $j$. We use gradient descent with momentum to update the parameters iteratively.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0043b-6b9a-4851-b1ff-ce4fef45e091",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources such as sample datasets, and setting up any required constants. \n",
    "\n",
    "> The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Otherwise, packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc952ea-d4e4-449b-aec7-ddc44d741972",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc914a79-bf7f-4889-b1d1-54003da268d8",
   "metadata": {},
   "source": [
    "### Load the MNIST digits data set\n",
    "Before training and testing the FNN, we construct two datasets. First, we build a training dataset of images to estimate the model parameters, stored in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable. Next, we construct a test dataset to evaluate how well the FNN generalizes to data it has never seen, stored in the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n",
    "\n",
    "> __Data format:__ The first element of each tuple is the input data $\\mathbf{x}$ (the image pixels arranged as a vector), and the second element is the label (whether the image corresponds to digits 0–9).\n",
    ">\n",
    "> __Type considerations:__ The floating-point precision is `Float32` rather than the default `Float64` to reduce memory usage. The labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot), and the input data is stored as a vector rather than a matrix (even though the original image is a $28\\times 28$ matrix of grayscale values).\n",
    "\n",
    "Before loading the testing and training data, let's set some constants. The comment next to each constant describes its permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047ec2c5-0c96-4c94-ace8-7165162fe37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = 3000; # how many training examples of *each* number to include from the library\n",
    "number_of_test_examples = 500; # how many examples are we going to test on?\n",
    "number_of_training_examples = number_of_examples - number_of_test_examples; # how many training examples of *each* number to include from the library\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583d83-6353-450f-9a6f-2ec5b44185c8",
   "metadata": {},
   "source": [
    "#### Load training and testing images\n",
    "Unhide the code blocks below to see how we construct and populate the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable. \n",
    "\n",
    "We load `number_of_training_examples::Int` images into the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` and then later convert these to vector format by linearizing the $28\\times 28$ matrix of grayscale values into a vector of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7edf7d-4bf1-468b-9c15-dc847a3c4f49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "digits_image_dictionary = MyMNISTHandwrittenDigitImageDataset(number_of_examples = number_of_examples);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e53240",
   "metadata": {},
   "source": [
    "What's in the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c42c51-185f-48c6-8e28-c03f3c6fce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;235m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;233;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;242m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;234;48;5;245m▀\u001b[38;5;246;48;5;255m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;247m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;243;48;5;249m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;239;48;5;244m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;243;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;251m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;246;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;250m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;253;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;245m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;233;48;5;234m▀\u001b[38;5;239;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;244;48;5;236m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;255;48;5;233m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;245m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;246;48;5;235m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;246m▀\u001b[38;5;253;48;5;233m▀\u001b[38;5;255;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;255m▀\u001b[38;5;239;48;5;255m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;241m▀\u001b[38;5;241;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;250m▀\u001b[38;5;249;48;5;234m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;238;48;5;232m▀\u001b[38;5;252;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;233;48;5;248m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;242m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;253m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;251;48;5;233m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;247;48;5;255m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;235;48;5;255m▀\u001b[38;5;232;48;5;250m▀\u001b[38;5;232;48;5;251m▀\u001b[38;5;234;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;248;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;240;48;5;232m▀\u001b[38;5;249;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_image_dictionary[8][ :, :, 10] # how does the indexing work? This is the 10th example of the digit \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25563ff",
   "metadata": {},
   "source": [
    "Next, let's partition the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` into training and testing datasets. We randomly select `number_of_training_examples::Int` images per digit for training, and the remaining images are used for testing. In each case, we convert the $28\\times 28$ images into vector format by linearizing the matrix into a vector of 784 pixels.\n",
    "\n",
    "Let's start with the training dataset. \n",
    "\n",
    "> __What is vectorization?__ Each $N\\times N$ image array containing grayscale values at each pixel is converted to an $N^{2}$ vector by concatenating pixel values. The image class (the digit it represents) is converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). \n",
    ">\n",
    "> __Why Float32?__ Most neural network libraries use `Float32` (or lower precision) to save memory because of the large number of parameters in the network. Additionally, model training is often carried out using specialized hardware such as [Graphical Processing Units (GPUs)](https://www.nvidia.com/en-us/data-center/h100/), which have different memory constraints.\n",
    "\n",
    "Let's save the training data in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d952896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000-element Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}:\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03137255, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007843138, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529412  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007843138  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015686275  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529412  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050980393  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529412, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ⋮\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015686275, 0.011764706  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529412  …  0.015686275, 0.007843138, 0.011764706, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.023529412, 0.0, 0.0, 0.023529412, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764706, 0.007843138  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529412, 0.003921569  …  0.0, 0.0, 0.07450981, 0.050980393, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007843138  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02745098, 0.003921569  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043137256, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "\n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec |> x -> convert.(Float32,x); # flatten and convert to Float32\n",
    "            training_tuple = (D,Y); # create training tuple (image data, image class)\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "\n",
    "    end\n",
    "    training_image_dataset; # return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb21729",
   "metadata": {},
   "source": [
    "What's in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cb8d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dataset[1] # what does the first training example look like? (image data, one-hot label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c33fc",
   "metadata": {},
   "source": [
    "Next, we load `number_of_test_examples::Int` images from the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` for testing purposes. We save the test data in the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fa21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_dataset = let\n",
    "    \n",
    "    # initialize -\n",
    "    testing_image_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}()\n",
    "    \n",
    "    # main -\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "        \n",
    "        for t ∈ (number_of_training_examples+1):number_of_examples\n",
    "\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec |> x -> convert.(Float32,x); # flatten and convert to Float32\n",
    "            testing_tuple = (D,Y); # create testing tuple (image data, image class)\n",
    "            push!(testing_image_dataset, testing_tuple);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    testing_image_dataset; # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edc74e",
   "metadata": {},
   "source": [
    "What's in the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76135db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Bool[1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_image_dataset[1] # what does the first test example look like? (image data, one-hot label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba2558",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18499f7b-5bd3-4a58-939c-633f9831df09",
   "metadata": {},
   "source": [
    "## Task 1: Build the model structure and train the network\n",
    "In this task, we construct and train a feedforward model by learning the model parameters using example images encoded in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}`. \n",
    "\n",
    "> __Supervised batch learning:__ This is an example of supervised batch learning where the model learns its parameters on a fixed set of labeled examples. Unlike online learning approaches (such as the perceptron), there is no incremental update rule. Once the parameters are estimated, new data cannot be easily incorporated without retraining.\n",
    "\n",
    "Let's start by getting the `number_of_input_states::Int`; this will be the input dimension of the first layer in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7f536e-0bc5-43c1-8258-7ab7e13ead59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_input_states = length(training_image_dataset[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8b2-fde1-4a36-8a32-1f4d59ba39c7",
   "metadata": {},
   "source": [
    "Next, we build an empty model with default (random) parameter values but a fixed structure. The number and dimension of the layers and the activation functions for each layer are specified when we build the model (but we update the parameters during training).\n",
    "\n",
    "> __Library:__ We use [the Flux.jl machine learning library](https://github.com/FluxML/Flux.jl) to construct the neural network model. The model has three layers: an input layer of dimension $784\\times 512$ with [ReLU activation functions](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), a hidden layer of dimension $512\\times 10$, and an output layer using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    ">\n",
    "> __Syntax:__ The Flux.jl package uses specialized syntax. The model is built using [the `Chain` function](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Chain), which takes a list of layers as input. Each layer is defined using the [`Dense` type](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Dense), which takes the number of input and output neurons as arguments. The activation function is an additional argument to the `Dense` type. The final layer uses [the `softmax(...)` method exported by the NNlib.jl package](https://fluxml.ai/NNlib.jl/dev/reference/#Softmax) to produce a probability distribution over the classes.\n",
    "\n",
    "We save the (untrained) model in the `model::Chain` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf74b6b-8aea-494b-b805-159d061acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@layer MyFluxNeuralNetworkModel; # create a layer type for our model\n",
    "MyModel() = MyFluxNeuralNetworkModel( # define a constructor function\n",
    "    Chain(\n",
    "        Dense(number_of_input_states, 512, relu),  # layer 1\n",
    "        Dense(512, 10, relu), # layer 2\n",
    "        NNlib.softmax) # layer 3 (output layer)\n",
    ");\n",
    "model = MyModel().chain; # this is our model with untrained parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f73bc-28ba-4a9e-af0b-ebc0d42392b0",
   "metadata": {},
   "source": [
    "Next, we specify the loss function we will minimize to estimate the model parameters. We choose a loss function appropriate for a multiclass classification problem, namely a [logit cross-entropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e432c99f-b8a5-4159-b1f3-5b77667c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a loss function -\n",
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; \n",
    "    agg = mean); # loss for training multiclass classifiers, what is the agg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a113-f473-4c02-a409-da18ccb088e7",
   "metadata": {},
   "source": [
    "We use [gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) where the $\\lambda$ parameter denotes the learning rate and $\\beta$ denotes the momentum parameter. We save information about the optimizer in the `opt_state` variable, which will be passed to the training method.\n",
    "\n",
    "> __Why this optimizer?__ We could have chosen from many possible training approaches. The [Flux.jl library supports many optimizers](https://fluxml.ai/Flux.jl/stable/reference/training/optimisers/#Optimisers-Reference), all of which are variants of gradient descent. However, there is no technical reason we couldn't use an optimizer that doesn't rely on computing the gradient of the loss function. For example, we could use a genetic algorithm or other optimization method. However, these methods are typically less efficient than gradient descent for this type of problem.\n",
    "\n",
    "This approach works well for our purposes. We'll explore more sophisticated training strategies in CHEME-5820!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368796dc-7bd9-4c3a-8d88-4092820393f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.01; # learning rate\n",
    "β = 0.90; # momentum parameter\n",
    "opt_state = Flux.setup(Momentum(λ, β), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57be96-4f45-4536-8e86-1d57a2caa24f",
   "metadata": {},
   "source": [
    "__Training:__ We are now ready to train the model. If the flag `should_we_train::Bool = true`, we use [gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) to minimize the [logit cross-entropy loss function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy).\n",
    "\n",
    "> __Restart strategy:__ Because the error landscape is non-convex, we need to explore from different starting points. We perform `number_of_epochs::Int` passes through the data, each consisting of a forward pass for prediction and a backpropagation step for parameter updates.\n",
    ">\n",
    "> __Checkpointing:__ Training takes considerable time. For each complete pass through the data (each epoch), we save a temporary file holding the network state as a checkpoint. A pre-trained model is loaded if the `should_we_train::Bool` flag is set to `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0e9b70-f312-4e3e-8693-c8d3350a4804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 512, relu),              \u001b[90m# 401_920 parameters\u001b[39m\n",
       "  Dense(512 => 10, relu),               \u001b[90m# 5_130 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m407_050 parameters, 1.553 MiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "should_we_train = false; # set this flag to {true | false}\n",
    "if (should_we_train == true)\n",
    "    number_of_epochs = 250; # how many epochs do we want to train for?\n",
    "    \n",
    "    @showprogress dt=1 desc=\"Training model...\" for i = 1:number_of_epochs\n",
    "        \n",
    "        # train the model -\n",
    "        Flux.train!(model, training_image_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y)\n",
    "        end\n",
    "    \n",
    "        # output some stuff -\n",
    "        ridx = rand(1:number_of_training_examples);\n",
    "        test_x, test_y = training_image_dataset[ridx][1], training_image_dataset[ridx][2];\n",
    "        l = loss(model(test_x), test_y);\n",
    "        println(\"Training example: $(ridx) has loss = $(l) in epoch $(i)\");\n",
    "    \n",
    "        # save the state of the model, in case something happens. We can reload from this state\n",
    "        jldsave(\"tmp-model-training-checkpoint.jld2\", model_state = Flux.state(model))    \n",
    "    end\n",
    "else\n",
    "    # if we don't train: load up a previous model\n",
    "    model_state = JLD2.load(\"model-state-T3000-P500-E250-N512.jld2\", \"model_state\");\n",
    "    Flux.loadmodel!(model, model_state);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a5e10",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d572-aa6b-4dda-88ea-c52d2a3bc495",
   "metadata": {},
   "source": [
    "## Task 2: How well does the model predict unseen versus observed images?\n",
    "In this task, we assess the network's generalization by measuring how well it performs on data it has not seen. One challenge with [neural networks](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) is their potential lack of generalizability: they may not perform well on data not observed during training.\n",
    "\n",
    "Let's explore this question by computing the fraction of images correctly classified on both the training and test datasets. We expect high accuracy on training data since the model has seen these examples. For the test data (unseen examples), we expect the accuracy to be at most equal to the training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98398ccb-1ec6-40c7-9586-f3ad475011f1",
   "metadata": {},
   "source": [
    "### Correct prediction on training dataset\n",
    "In the code block below, we pass the pixel data from each image into the `model::Chain` instance, compute the predicted label $\\hat{y}$, and compare the predicted and actual labels for the training dataset.\n",
    "\n",
    "> __Evaluation logic:__ If the prediction and actual label agree, we update the running count of correct predictions. We then compute the fraction of correct classifications by dividing the number of correct predictions by the total number of images in the training dataset.\n",
    "\n",
    "So, how well did we do on the __training dataset__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67a99f9-5efe-48de-a8f8-d9962dff24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction % on the training data: 89.94%\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    S_training = 0;\n",
    "    for i ∈ eachindex(training_image_dataset)\n",
    "    \n",
    "        x = training_image_dataset[i][1];\n",
    "        y = training_image_dataset[i][2];\n",
    "        ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z,number_digit_array)\n",
    "        y == ŷ ? S_training +=1 : nothing\n",
    "    end\n",
    "    correct_prediction_training = (S_training/length(training_image_dataset))*100;\n",
    "    println(\"Correct prediction % on the training data: $(correct_prediction_training)%\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ececb",
   "metadata": {},
   "source": [
    "### Correct prediction on test dataset\n",
    "In the code block below, we pass the pixel data from each image into the `model::Chain` instance, compute the predicted label $\\hat{y}$, and compare the predicted and actual labels for the test dataset. \n",
    "\n",
    "> __Evaluation logic:__ If the prediction and actual label agree, we update the running count of correct predictions. We then compute the fraction of correct classifications by dividing the number of correct predictions by the total number of images in the test dataset.\n",
    "\n",
    "Is the model test accuracy comparable to the training accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a021a683-8441-469d-8981-0c48800cf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction on the test data: 89.9%\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    S_testing = 0;\n",
    "\n",
    "    # count correct predictions -\n",
    "    for i ∈ eachindex(testing_image_dataset)\n",
    "    \n",
    "        x = testing_image_dataset[i][1];\n",
    "        y = testing_image_dataset[i][2];\n",
    "        ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "        y == ŷ ? S_testing+=1 : nothing\n",
    "    end\n",
    "    correct_prediction_test = (S_testing/length(testing_image_dataset))*100;\n",
    "    println(\"Correct prediction on the test data: $(correct_prediction_test)%\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535531c-5361-4a4a-8f52-d8170be52f5b",
   "metadata": {},
   "source": [
    "### Examples\n",
    "Let's examine a few manual examples to see what the model predicts. In the code block below, we take random images from the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` and compare the predicted and actual labels in one-hot format. If the prediction is wrong, we display the actual image and the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "789967b5-7126-40a3-a57c-649c1cdff027",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    \n",
    "    N = length(testing_image_dataset); # how many test examples\n",
    "    i = rand(1:N) # select a random test example\n",
    "    x = testing_image_dataset[i][1]; # get the input for this test example\n",
    "    y = testing_image_dataset[i][2]; # get the *actual* output for this test example (onehot encoding)\n",
    "   \n",
    "    # compute onehot encoding of the predicted output -\n",
    "    ŷ = model(x) |> z-> argmax(z) |> z-> number_digit_array[z] |> z-> onehot(z, number_digit_array)\n",
    "    flag = y == ŷ # check that the predicted output is the same as the actual output\n",
    "    if (flag == false)\n",
    "        println(ŷ) # onehot encoding of the *predicted* output\n",
    "        display(reshape(x, number_of_rows, number_of_cols) |> X -> Gray.(transpose(X))); # actual image\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a14e5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34307dc3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This example demonstrates building, training, and evaluating a feedforward neural network for multiclass image classification using the MNIST dataset.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Feedforward networks for classification:** FNNs with dense layers and activation functions map vectorized image inputs to class probabilities. The softmax output layer converts raw scores into a probability distribution over the 10 digit classes.\n",
    "> * **Data preparation matters:** Converting images to vectorized Float32 format with one-hot encoded labels enables efficient batch training. Using Float32 precision reduces memory requirements for large networks.\n",
    "> * **Generalization assessment:** Comparing accuracy on training data versus unseen test data reveals whether the model has learned generalizable patterns or has overfit to the training examples.\n",
    "\n",
    "Feedforward neural networks provide a foundation for more complex architectures such as convolutional neural networks for image tasks and recurrent neural networks for sequential data.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.2",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
