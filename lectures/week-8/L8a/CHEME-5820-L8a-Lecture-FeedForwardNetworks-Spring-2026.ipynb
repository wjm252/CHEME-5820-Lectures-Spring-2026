{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9298eb4a-7abb-40a4-a9a0-aff9cfe005f1",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks (FNNs)\n",
    "In this lecture, we introduce feedforward neural networks (FNNs), artificial neural network architectures where connections between layers do not form cycles. This differs from recurrent neural networks (RNNs), where data can flow in cycles.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lecture, you will be able to:\n",
    ">\n",
    "> * __FNN Architecture__: Define feedforward neural networks as layered structures where information flows from input nodes through hidden layers to output nodes without cycles. Understand that each node applies a linear transformation followed by a non-linear activation function.\n",
    "> * __FNN as Function Composition__: Describe how an FNN computes a mapping from inputs to outputs as a series of function compositions, where each layer transforms the output of the previous layer.\n",
    "> * __FNN Training__: Explain how FNNs are trained using backpropagation, which computes gradients of a loss function with respect to network parameters using the chain rule, enabling gradient descent optimization.\n",
    "\n",
    "Sources for this lecture:\n",
    "* [John Hertz, Anders Krogh, and Richard G. Palmer. 1991. Introduction to the theory of neural computation. Addison-Wesley Longman Publishing Co., Inc., USA.](https://dl.acm.org/doi/10.5555/104000)\n",
    "* [Mehlig, B. (2021). Machine Learning with Neural Networks. Chapter 5: Perceptrons and Chapter 6: Stochastic Gradient Descent](https://arxiv.org/abs/1901.05639v4)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9f079",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"figs/nn-4.svg\"\n",
    "  alt=\"Feedforward neural network with input layer (5 nodes), hidden layer (12 nodes), and output layer (3 nodes)\"\n",
    "  height=\"400\"\n",
    "  width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2828e",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks\n",
    "Consider the network shown above. The network has three layers: an input layer (five nodes), a hidden layer (twelve nodes), and an output layer (three nodes). \n",
    "\n",
    "* __Input layer__: The input layer receives raw data, with each node representing one feature of the input vector. It does not perform computationâ€”its role is to pass input values to the next layer.\n",
    "*  __Hidden layer__: The hidden layer performs computations on the input data. Each node takes the input vector $\\mathbf{x}$ and applies a linear transformation followed by a non-linear activation function. The output of each node passes to the next layer. The number of nodes and layers can vary depending on the task complexity. Networks with multiple hidden layers are called _deep neural networks_.\n",
    "* __Output layer__: The output layer produces the network output $\\mathbf{y} = (y_{1},y_{2},\\dots,y_{k})$ where $\\mathbf{y}\\in\\mathbb{R}^{k}$. Each node takes output from the hidden layer and applies a linear transformation followed by an activation function. For multi-class classification, the output can be interpreted as probabilities for each class using softmax.\n",
    "\n",
    "### Function Composition\n",
    "Suppose we have a feedforward neural network with $L$ layers. The network has $d_{in}$ input nodes (layer 0), $i=1,2,\\dots, L-1$ hidden layers where hidden layer $i$ has $m_{i}$ nodes, and the output layer $L$ has $d_{out}$ nodes. Each hidden layer is fully connected to the previous and subsequent layers (no connections within a layer and no self-connections). Information flows from input to output, forming a feedforward structure.\n",
    "\n",
    "#### Inputs, Outputs, and Parameters\n",
    "* __Inputs and outputs__: Let $\\mathbf{x} = (x_{1},x_{2},\\dots,x_{d_{in}},1)^{\\top}$ be the _augmented_ input vector, where the extra `1` allows the bias term to be included in the weight vector. Let $\\mathbf{z}^{(i)} = (z^{(i)}_{1},z^{(i)}_{2},\\dots,z^{(i)}_{m_{i}})^{\\top}$ be the output vector of layer $i$. Finally, let $\\hat{\\mathbf{y}}= (y_{1},y_{2},\\dots,y_{d_{out}})^{\\top}$ be the predicted output vector.\n",
    "\n",
    "* __Parameters__: Each node $j=1,2,\\dots,m_{i}$ in layer $i\\geq{1}$ has a parameter vector $\\mathbf{w}^{(i)}_{j} = (w^{(i)}_{j,1},w^{(i)}_{j,2},\\dots,w^{(i)}_{j,m_{i-1}}, b^{(i)}_{j})^{\\top}$, where $w^{(i)}_{j,k}\\in\\mathbb{R}$ is the weight connecting node $k$ in layer $i-1$ to node $j$ in layer $i$, and $b^{(i)}_{j}\\in\\mathbb{R}$ is the bias term for node $j$ in layer $i$.\n",
    "\n",
    "#### Layer Computations\n",
    "A feedforward neural network computes a series of function compositions. For layer $1$ with $m_{1}$ nodes, the output given input $\\mathbf{z}^{(0)} = \\mathbf{x}$ is:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{z}^{(1)} = \\begin{pmatrix}\n",
    "\\sigma_{1}\\left((\\mathbf{z}^{(0)})^{\\top}\\mathbf{w}^{(1)}_{1}\\right) \\\\\n",
    "\\sigma_{1}\\left((\\mathbf{z}^{(0)})^{\\top}\\mathbf{w}^{(1)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{1}\\left((\\mathbf{z}^{(0)})^{\\top}\\mathbf{w}^{(1)}_{m_{1}}\\right)\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{w}^{(1)}_{j}$ is the parameter vector for node $j$ in layer $1$, and $\\sigma_{1}$ is the activation function for layer $1$. The output $\\mathbf{z}^{(1)}\\in\\mathbb{R}^{m_{1}}$ passes to layer $2$, which has $m_{2}$ nodes:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{z}^{(2)} = \\begin{pmatrix}\n",
    "\\sigma_{2}\\left((\\mathbf{z}^{(1)})^{\\top}\\mathbf{w}^{(2)}_{1}\\right) \\\\\n",
    "\\sigma_{2}\\left((\\mathbf{z}^{(1)})^{\\top}\\mathbf{w}^{(2)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{2}\\left((\\mathbf{z}^{(1)})^{\\top}\\mathbf{w}^{(2)}_{m_{2}}\\right)\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We can write the output of layer $2$ as: $\\mathbf{z}^{(2)} = \\sigma_{2}\\circ\\sigma_{1}(\\mathbf{z}^{(0)})$, where $\\sigma_{2}\\circ\\sigma_{1}$ is the composition of the two layer functions. Generalizing, a feedforward neural network computes:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{y}} = f_{\\theta}(\\mathbf{x}) = \\sigma_{L}\\circ\\sigma_{L-1}\\circ\\dots\\circ\\sigma_{1}(\\mathbf{x})\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\sigma_{L}$ is the activation function for the output layer, $\\mathbf{x}$ is the augmented input vector, $\\hat{\\mathbf{y}}\\in\\mathbb{R}^{d_{out}}$ is the network output, and $\\theta$ represents all network parameters (weights and biases).\n",
    "\n",
    "> __Key Insight__: A feedforward neural network $f_{\\theta}:\\mathbb{R}^{d_{in}}\\rightarrow\\mathbb{R}^{d_{out}}$ is a function that maps input $\\mathbf{x}$ to output $\\hat{\\mathbf{y}}$. We can apply all the tools of calculus and linear algebra to analyze and optimize neural networks: compose them, differentiate them, and optimize their parameters.\n",
    "\n",
    "### Parameterization\n",
    "The parameters of layer $i$ can be represented as the matrix $\\mathbf{W}_{i}\\in\\mathbb{R}^{m_{i}\\times(m_{i-1}+1)}$, where each row contains the weights and bias for one node. All parameters $(\\mathbf{W}_{1},\\mathbf{W}_{2},\\dots,\\mathbf{W}_{L})$ can be packed into a single vector $\\theta$:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta \\equiv  (w^{(1)}_{1,1},\\dots,w^{(1)}_{1,m_{0}}, b^{(1)}_{1}, w^{(1)}_{2,1},\\dots, b^{(1)}_{2}, \\ldots, w^{(L)}_{d_{out},m_{L-1}}, b^{(L)}_{d_{out}})\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### How Many Parameters?\n",
    "The number of parameters in a feedforward neural network depends on the number of layers and nodes per layer. For a network with $L$ layers where layer $i$ has $m_{i}$ nodes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Number of parameters} &= \\sum_{i=1}^{L} m_{i}(m_{i-1}+1) \\\\\n",
    "&= \\underbrace{\\sum_{i=1}^{L} m_{i} m_{i-1}}_{\\text{weights}} + \\underbrace{\\sum_{i=1}^{L} m_{i}}_{\\text{biases}} \n",
    "\\end{align*}\n",
    "$$\n",
    "where $m_{0} = d_{in}$ is the number of input features.\n",
    "\n",
    "__Where do these parameters come from?__ The parameters are learned through a gradient descent algorithm called __backpropagation__ by minimizing a loss function appropriate for the task (e.g., regression or classification).\n",
    "\n",
    "Let's explore [how backpropagation works](CHEME-146-M1-FFN-Training-Watch-Demo.ipynb).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e01c0c",
   "metadata": {},
   "source": [
    "## Strengths and Weaknesses of FNNs\n",
    "\n",
    "### Strengths\n",
    "* __Universal Approximation__: [FNNs are universal approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem), meaning they can approximate any continuous function to arbitrary precision given sufficient hidden units and training data.\n",
    "* __Flexibility__: FNNs can be applied to classification, regression, and generative modeling tasks. They can work with different data types including images, text, and time series. \n",
    "* __Non-linearity__: Non-linear activation functions enable FNNs to learn complex non-linear relationships that linear models cannot capture. They can classify non-linearly separable datasets.\n",
    "\n",
    "### Weaknesses\n",
    "* __Overfitting__: FNNs can overfit training data, especially when the model is complex or training data is limited. Regularization techniques (dropout, L1/L2 regularization, early stopping) can mitigate this issue.\n",
    "* __Computational Cost__: Training FNNs can be computationally expensive for large datasets or deep networks. Modern hardware and software frameworks help, but efficient implementation requires expertise.\n",
    "* __Interpretability__: FNNs are often considered __black box__ models, making it difficult to interpret predictions. This is a disadvantage in applications where interpretability is required (e.g., healthcare, finance). Techniques such as [LIME](https://arxiv.org/abs/1602.04938) and [SHAP](https://arxiv.org/abs/1705.07874) can help improve interpretability.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ace57",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Feedforward neural networks are layered architectures where information flows from input to output without cycles, with each node computing a linear transformation followed by a non-linear activation function.\n",
    "\n",
    "> __Key Takeaways__:\n",
    ">\n",
    "> * __FNN Architecture__: Feedforward neural networks consist of input, hidden, and output layers connected in a directed acyclic structure. Each node applies a weighted sum followed by a non-linear activation function. The network computes a series of function compositions, where each layer transforms the output of the previous layer.\n",
    "> * __Training via Backpropagation__: FNNs are trained using backpropagation, which computes gradients of a loss function with respect to network parameters using the chain rule. Stochastic gradient descent and mini-batch variants provide computationally efficient approaches to parameter optimization.\n",
    "> * __Universal Approximation with Trade-offs__: FNNs can approximate any continuous function given sufficient capacity, making them applicable to classification, regression, and other tasks. However, they are prone to overfitting, computationally expensive to train, and often lack interpretability.\n",
    "\n",
    "The concepts introduced here, layered architectures, activation functions, and gradient-based training, form the foundation for more advanced neural network architectures that we will explore next semester.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
